{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenisation of Text into sentences\n",
    "Code from [Mastering Natural Langiage Processing with Python](https://www.packtpub.com/big-data-and-business-intelligence/mastering-natural-language-processing-python). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text=\"Welcome readers. I hope you find it interesting. please fo reply.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Welcome readers.', 'I hope you find it interesting.', 'please fo reply.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For large number of senteces try this instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text='Hello everyone. Hope you are all fine and doing well. Hope that you will find the book interesting.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello everyone.',\n",
       " 'Hope you are all fine and doing well.',\n",
       " 'Hope that you will find the book interesting.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For splitting sentences into words\n",
    "This can be done using the `word_tokenize` as per below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text=nltk.word_tokenize(\"PeirreVinken, 59 years old, will join as a nonexecutive director on Nov. 29.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PeirreVinken', ',', '59', 'years', 'old', ',', 'will', 'join', 'as', 'a', 'nonexecutive', 'director', 'on', 'Nov.', '29', '.']\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide some imput text:\n"
     ]
    }
   ],
   "source": [
    "r=input(\"Please provide some imput text:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of this word is 0 words.\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "print(\"The length of this word is\", len(word_tokenize(r)),\"words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What about other tokenizers\n",
    "There is the `TreeBankTokenizer` which tokenises according to Penn Treebank Corpus conventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Have', 'a', 'nice', 'day.', 'I', 'hope', 'you', 'find', 'the', 'book', 'interesting']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "print(tokenizer.tokenize(\"Have a nice day. I hope you find the book interesting\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', \"n't\", 'hesitate', 'to', 'ask', 'questions']\n"
     ]
    }
   ],
   "source": [
    "text=nltk.word_tokenize(\" Don't hesitate to ask questions\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What about using regular expressions\n",
    "Regular expressions can be used to split text based on punctuation chars. Luckily, we won't need RegExs just yet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', \"'\", 't', 'hesitate', 'to', 'ask', 'questions']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer=WordPunctTokenizer()\n",
    "print(tokenizer.tokenize(\" Don't hesitate to ask questions\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if you want to use regular expressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Don't\", 'hesitate', 'to', 'ask', 'questions']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer=RegexpTokenizer(\"[\\w']+\")\n",
    "print(tokenizer.tokenize(\"Don't hesitate to ask questions\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of instantiating a class, here is an alternative way to tokenize with regexs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', \"'t\", 'hesitate', 'to', 'ask', 'questions']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "sent=\"Don't hesitate to ask questions\"\n",
    "print(regexp_tokenize(sent, pattern='\\w+|\\$[\\d\\.]+|\\S+'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is how we can "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Don't\", 'hesitate', 'to', 'ask', 'questions', 'Hi', 'there', 'Antoio-b', 'was', '54', 'years', 'old']\n"
     ]
    }
   ],
   "source": [
    "tokenizer=RegexpTokenizer('[\\'\\w\\-]+',gaps=False)\n",
    "print(tokenizer.tokenize(\"Don't hesitate to ask questions. Hi there! Antoio-b was 54 years old\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about selecting words with a capital letter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['She', 'She']\n"
     ]
    }
   ],
   "source": [
    "sent=\" She secured 90.56 % in class X . She is a meritorious student\"\n",
    "capt = RegexpTokenizer('[A-Z]\\w+')\n",
    "print(capt.tokenize(sent))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other RegeEx tokenizers\n",
    "And what about using predefined regexs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' She secured 90.56 % in class X . She is a meritorious student']\n"
     ]
    }
   ],
   "source": [
    "sent=\" She secured 90.56 % in class X . She is a meritorious student\"\n",
    "from nltk.tokenize import BlanklineTokenizer\n",
    "print(BlanklineTokenizer().tokenize(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenisation using a white_space tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['She', 'secured', '90.56', '%', 'in', 'class', 'X', '.', 'She', 'is', 'a', 'meritorious', 'student']\n"
     ]
    }
   ],
   "source": [
    "sent=\" She secured 90.56 % in class X . She is a meritorious student\"\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "print(WhitespaceTokenizer().tokenize(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The split method can also be used to specify a white_space character or anything else:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['She', 'secured', '90.56', '%', 'in', 'class', 'X.', 'She', 'is', 'a', 'meritorious', 'student']\n",
      "['She', 'secured', '90.56', '%', 'in', 'class', 'X.', 'She', 'is', 'a', 'meritorious', 'student']\n",
      "[' She secured 90.56 % in class X ', '. She is a meritorious student', '']\n"
     ]
    }
   ],
   "source": [
    "sent= \"She secured 90.56 % in class X. She is a meritorious student\"\n",
    "print(sent.split())     # Notice that these two...\n",
    "print(sent.split(' '))  # ...are equivalent\n",
    "sent=\" She secured 90.56 % in class X \\n. She is a meritorious student\\n\"\n",
    "print(sent.split('\\n'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `SpaceTokenizer` works in a very similar way to `sent.split(' ')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'She', 'secured', '90.56', '%', 'in', 'class', 'X', '\\n.', 'She', 'is', 'a', 'meritorious', 'student\\n']\n"
     ]
    }
   ],
   "source": [
    "sent=\" She secured 90.56 % in class X \\n. She is a meritorious student\\n\"\n",
    "from nltk.tokenize import SpaceTokenizer\n",
    "print(SpaceTokenizer().tokenize(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if all we want it to tokenize words into lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' She secured 90.56 % in class X \\n. She is a meritorious student\\n']\n",
      "[' She secured 90.56 % in class X ', '. She is a meritorious student']\n",
      "[' She secured 90.56 % in class X ', '. She is a meritorious student']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import BlanklineTokenizer\n",
    "sent=\" She secured 90.56 % in class X \\n. She is a meritorious student\\n\"\n",
    "print(BlanklineTokenizer().tokenize(sent))\n",
    "from nltk.tokenize import LineTokenizer\n",
    "print(LineTokenizer(blanklines='keep').tokenize(sent))\n",
    "print(LineTokenizer(blanklines='discard').tokenize(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is the process of: \n",
    "1. eliminating punctuation, \n",
    "2. converting the entire text into lowercase or uppercase, \n",
    "3. changing numbers into words, \n",
    "4. expanding abbreviations, \n",
    "5. canonicalisation of text \n",
    "and so on.\n",
    "\n",
    "Let's start with punctuation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['It', 'is', 'a', 'pleasant', 'evening'], ['Guests', 'who', 'came', 'from', 'US', 'arrived', 'at', 'the', 'venue'], ['Food', 'was', 'tasty']]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "text=[\" It is a pleasant evening.\",\"Guests, who came from US arrived at the venue\",\"Food was tasty.\"]\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_docs=[word_tokenize(doc) for doc in text]\n",
    "x=re.compile('[%s]' % re.escape(string.punctuation))\n",
    "tokenized_docs_no_punctuation = []\n",
    "for review in tokenized_docs:\n",
    "    new_review = []\n",
    "    for token in review: \n",
    "        new_token = x.sub(u'', token)\n",
    "        if not new_token == u'':\n",
    "            new_review.append(new_token)\n",
    "    tokenized_docs_no_punctuation.append(new_review)\t\n",
    "print(tokenized_docs_no_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert text to lowercase and uppercase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardwork is key to success\n",
      "HARDWORK IS KEY TO SUCCESS\n"
     ]
    }
   ],
   "source": [
    "text='HARdWork IS KEy to SUCCESS'\n",
    "print(text.lower())\n",
    "print(text.upper())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords\n",
    "Also, another important part is stopwords. Stopwords are words that need to be filtered out as they appear way too often and do not provide too much useful information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Don't\", 'hesitate', 'ask', 'questions']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stops=set(stopwords.words('english'))\n",
    "words=[\"Don't\", 'hesitate','to','ask','questions']\n",
    "print([word for word in words if word not in stops])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arabic', 'azerbaijani', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'greek', 'hungarian', 'indonesian', 'italian', 'kazakh', 'nepali', 'norwegian', 'portuguese', 'romanian', 'russian', 'slovene', 'spanish', 'swedish', 'tajik', 'turkish']\n"
     ]
    }
   ],
   "source": [
    "# For other languages:\n",
    "print(stopwords.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# Lets calculate stopwords in english\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def para_fraction(text):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    para = [w for w in text if w.lower() not in stopwords]\n",
    "    return len(para) / len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mreuters\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('reuters')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/reuters\u001b[0m\n\n  Searched in:\n    - '/Users/chadjinik/nltk_data'\n    - '/Users/chadjinik/anaconda3/nltk_data'\n    - '/Users/chadjinik/anaconda3/share/nltk_data'\n    - '/Users/chadjinik/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}/{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mreuters\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('reuters')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/reuters.zip/reuters/\u001b[0m\n\n  Searched in:\n    - '/Users/chadjinik/nltk_data'\n    - '/Users/chadjinik/anaconda3/nltk_data'\n    - '/Users/chadjinik/anaconda3/share/nltk_data'\n    - '/Users/chadjinik/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-432bdef9b71f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpara_fraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreuters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}/{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}/{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    583\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mreuters\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('reuters')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/reuters\u001b[0m\n\n  Searched in:\n    - '/Users/chadjinik/nltk_data'\n    - '/Users/chadjinik/anaconda3/nltk_data'\n    - '/Users/chadjinik/anaconda3/share/nltk_data'\n    - '/Users/chadjinik/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "print(para_fraction(nltk.corpus.reuters.words()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93minaugural\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('inaugural')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/inaugural\u001b[0m\n\n  Searched in:\n    - '/Users/chadjinik/nltk_data'\n    - '/Users/chadjinik/anaconda3/nltk_data'\n    - '/Users/chadjinik/anaconda3/share/nltk_data'\n    - '/Users/chadjinik/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}/{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93minaugural\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('inaugural')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/inaugural.zip/inaugural/\u001b[0m\n\n  Searched in:\n    - '/Users/chadjinik/nltk_data'\n    - '/Users/chadjinik/anaconda3/nltk_data'\n    - '/Users/chadjinik/anaconda3/share/nltk_data'\n    - '/Users/chadjinik/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-6ff8ba603d7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpara_fraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minaugural\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}/{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}/{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    583\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93minaugural\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('inaugural')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/inaugural\u001b[0m\n\n  Searched in:\n    - '/Users/chadjinik/nltk_data'\n    - '/Users/chadjinik/anaconda3/nltk_data'\n    - '/Users/chadjinik/anaconda3/share/nltk_data'\n    - '/Users/chadjinik/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "print(para_fraction(nltk.corpus.inaugural.words()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Substituting and correcting tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot is a contradicton\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "replacement_patterns = [\n",
    "(r'won\\'t', 'will not'),\n",
    "(r'can\\'t', 'cannot'),\n",
    "(r'i\\'m', 'i am'),\n",
    "(r'ain\\'t', 'is not'),\n",
    "(r'(\\w+)\\'ll', '\\g<1> will'),\n",
    "(r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "(r'(\\w+)\\'ve', '\\g<1> have'),\n",
    "(r'(\\w+)\\'s', '\\g<1> is'),\n",
    "(r'(\\w+)\\'re', '\\g<1> are'),\n",
    "(r'(\\w+)\\'d', '\\g<1> would')\n",
    " ]\n",
    "\n",
    "class RegexpReplacer(object):\n",
    "\n",
    "   def __init__(self, patterns=replacement_patterns):\n",
    "\n",
    "      # Fixed this line - \"patterns\", not \"pattern\"\n",
    "      self.patterns = [(re.compile(regex), repl) for (regex, repl) in patterns]\n",
    "\n",
    "   def replace(self, text):\n",
    "      s = text\n",
    "      for (pattern, repl) in self.patterns:\n",
    "          (s, count) = re.subn(pattern, repl, s)\n",
    "\n",
    "      # Fixed indentation here\n",
    "      return s\n",
    "\n",
    "\n",
    "rep=RegexpReplacer()\n",
    "print(rep.replace(\"can't is a contradicton\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She must have gone to the market but she did not go\n"
     ]
    }
   ],
   "source": [
    "replacer= RegexpReplacer()\n",
    "replacer.replace(\"Don't hesitate to ask questions\")\n",
    "print(replacer.replace(\"She must've gone to the market but she didn't go\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', 'not', 'hesitate', 'to', 'ask', 'questions']\n"
     ]
    }
   ],
   "source": [
    "# Substitution can take place before tokenisation\n",
    "replacer=RegexpReplacer()\n",
    "word_tokenize(\"Don't hesitate to ask questions\")\n",
    "print(word_tokenize(replacer.replace(\"Don't hesitate to ask questions\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete repeating characters\n",
    "Have a look at [this](https://github.com/japerk/nltk3-cookbook/blob/master/replacers.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "class RepeatReplacer(object):\n",
    "\t\"\"\" Removes repeating characters until a valid word is found.\n",
    "\t>>> replacer = RepeatReplacer()\n",
    "\t>>> replacer.replace('looooove')\n",
    "\t'love'\n",
    "\t>>> replacer.replace('oooooh')\n",
    "\t'ooh'\n",
    "\t>>> replacer.replace('goose')\n",
    "\t'goose'\n",
    "\t\"\"\"\n",
    "\tdef __init__(self):\n",
    "\t\tself.repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "\t\tself.repl = r'\\1\\2\\3'\n",
    "\n",
    "\tdef replace(self, word):\n",
    "\t\tif wordnet.synsets(word):\n",
    "\t\t\treturn word\n",
    "\t\t\n",
    "\t\trepl_word = self.repeat_regexp.sub(self.repl, word)\n",
    "\t\t\n",
    "\t\tif repl_word != word:\n",
    "\t\t\treturn self.replace(repl_word)\n",
    "\t\telse:\n",
    "\t\t\treturn repl_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lot\n",
      "oh\n",
      "ooh\n"
     ]
    }
   ],
   "source": [
    "replacer=RepeatReplacer()\n",
    "print(replacer.replace('lotttt'))\n",
    "print(replacer.replace('ohhhhh'))\n",
    "print(replacer.replace('ooohhhhh'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replacing words witht heir synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WordReplacer(object):\n",
    "\t\"\"\" WordReplacer that replaces a given word with a word from the word_map,\n",
    "\tor if the word isn't found, returns the word as is.\n",
    "\t>>> replacer = WordReplacer({'bday': 'birthday'})\n",
    "\t>>> replacer.replace('bday')\n",
    "\t'birthday'\n",
    "\t>>> replacer.replace('happy')\n",
    "\t'happy'\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, word_map):\n",
    "\t\tself.word_map = word_map\n",
    "\t\n",
    "\tdef replace(self, word):\n",
    "\t\treturn self.word_map.get(word, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "congratulations\n",
      "maths\n"
     ]
    }
   ],
   "source": [
    "replacer=WordReplacer({'congrats':'congratulations'})\n",
    "print(replacer.replace('congrats'))\n",
    "print(replacer.replace('maths'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zipfs Law\n",
    "It states that the frequency of a token in a text is directly proportional to its rank or position in the sorted list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/chadjinik/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feee0143fd0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAESCAYAAADuVeJ5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRAKihCEiIgFRwhARkYAoYYiISED+H+6tci+oLPn/AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feeea3b6080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feee0143fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.probability import FreqDist\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "fd = FreqDist()\n",
    "for text in gutenberg.fileids():\n",
    "    for word in gutenberg.words(text):\n",
    "        fd[word] += 1\n",
    "\n",
    "ranks = []\n",
    "freqs = []\n",
    "for rank, word in enumerate(fd):\n",
    "    ranks.append(rank+1)\n",
    "    freqs.append(fd[word])\n",
    "\n",
    "plt.loglog(ranks,freqs)\n",
    "plt.xlabel('frequency(f)', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('rank(r)', fontsize=14, fontweight='bold')\n",
    "plt.grid(True)\n",
    "fig = plt.figure(figsize=(40,20), dpi= 80, facecolor='w', edgecolor='k')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Similarity Measures\n",
    "Many similarity measures can be used to quantify the similarity between words, phrases or bigger chanks of text. The `nltk.metrics` lib is used for this task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.6666666666666666\n",
      "Recall:0.6666666666666666\n",
      "F_measure:0.8\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from nltk.metrics import *\n",
    "training='PERSON OTHER PERSON OTHER OTHER ORGANIZATION'.split()\n",
    "testing='PERSON OTHER OTHER OTHER OTHER OTHER'.split()\n",
    "print('Accuracy:' + str(accuracy(training,testing)))\n",
    "trainset=set(training)\n",
    "testset=set(testing)\n",
    "precision(trainset,testset)\n",
    "print('Recall:' + str(recall(trainset,testset)))\n",
    "print('F_measure:' + str(f_measure(trainset,testset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edit distance\n",
    "Minimum number of insertions, deletions or substitutions necessary to convert one word into another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "from nltk.metrics import *\n",
    "print(edit_distance(\"relate\",\"relation\"))\n",
    "print(edit_distance(\"suggestion\",\"calculation\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jaccard Coefficient\n",
    "A measure of the overlap between two sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6\n"
     ]
    }
   ],
   "source": [
    "X=set([10,20,30,40])\n",
    "Y=set([20,30,60])\n",
    "print(jaccard_distance(X,Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Similarity algorithms\n",
    "The binary distance which returns 0 if the two strings are exactly the same and 1 if they are not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "X = set([10,20,30,40])\n",
    "Y= set([30,50,70])\n",
    "print(binary_distance(X, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or the masi distance, which is based on partial agreement when multiple labels are present:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.945\n"
     ]
    }
   ],
   "source": [
    "X=set([10,20,30,40])\n",
    "Y=set([30,50,70])\n",
    "print(masi_distance(X,Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
