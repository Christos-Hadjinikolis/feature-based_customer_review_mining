{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: autocorrect in /Users/chadjinik/anaconda3/lib/python3.6/site-packages (2.0.0)\r\n"
     ]
    }
   ],
   "source": [
    "# !pip install autocorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For monitoring duration of pandas processes\n",
    "from tqdm import tqdm, tqdm_pandas\n",
    "\n",
    "# To avoid RuntimeError: Set changed size during iteration\n",
    "tqdm.monitor_interval = 0\n",
    "\n",
    "# Register `pandas.progress_apply` and `pandas.Series.map_apply` with `tqdm`\n",
    "# (can use `tqdm_gui`, `tqdm_notebook`, optional kwargs, etc.)\n",
    "tqdm.pandas(desc=\"Progress:\")\n",
    "\n",
    "# Now you can use `progress_apply` instead of `apply`\n",
    "# and `progress_map` instead of `map`\n",
    "# can also groupby:\n",
    "# df.groupby(0).progress_apply(lambda x: x**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For visuals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %pip install python-decouple\n",
    "from decouple import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "API_USERNAME = config('USER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "API_KEY = config('PLOTLY_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import chart_studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chart_studio.tools.set_credentials_file(username=API_USERNAME, api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import chart_studio.plotly as py\n",
    "import plotly.offline\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cufflinks as cf\n",
    "cf.go_offline()\n",
    "# Configure cufflings \n",
    "cf.set_config_file(offline=False, world_readable=True, theme='pearl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield eval(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getDF(path):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    for d in parse(path):\n",
    "        df[i] = d\n",
    "        i += 1\n",
    "    return pd.DataFrame.from_dict(df, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = getDF('../data/raw/reviews_Books_5.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[['reviewerID','asin','reviewText','helpful']]\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new Column for the denominator and the enumerator\n",
    "df2 = df1.assign(enum = df1['helpful'].progress_apply(lambda enum_denom:enum_denom[0]))\n",
    "df3 = df2.assign(denom = df2['helpful'].progress_apply(lambda enum_denom:enum_denom[1]))\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Uniquekey Column\n",
    "df4 = df3.assign(uniqueKey = df3['reviewerID'].str.cat(df3['asin'].values.astype(str), sep='##'))\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing more Reviews based on their Helpfulness\n",
    "Summarisation is not usefull in all possible cases. For instance, if we only have 1 review for a book then when may as well just read the review rather than its summary. In addition, we need of a way to identify usefull **negative** and or **possitive** reviews. Thankfully, the `denominator` value in the helpful field can help us with that.\n",
    "\n",
    "Reviews that have less than 5 helpfulness ratings should be filtered out of the dataset. This is not to say that all reviews that are not scutinised enough are not good, but rather that we have no information as to whether they are usefull or not at all. As such we have no control over those reviews and would rather remove them so as to make more informed decisions on the quality of the dataset later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df4.loc[df4['denom'] >  5]\n",
    "print(\"Remaining reviews: \" + str(len(df5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further understanding out dataset\n",
    "In order to gain a deeper understanding of the quality of our dataset we need to know if a given review is helpful or not. This can be achieved by dividing the `enumerator/denominator` numbers in the `helpful` field; that is meant to represent the ratio of the people who found the review useful over the total that rated the review. We will assume that if the ration is above 50% then the review is helpful else it is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set thrshold and calculate usefulness\n",
    "threshold = 50/100\n",
    "df6 = df5.assign(useful = np.where(df5.loc[:, 'enum'] / df5.loc[:, 'denom'] > threshold, True, False))\n",
    "df6.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's count the useful vs the not useful ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_table = df6.groupby('useful').count()\n",
    "stats_table = stats_table.reset_index()\n",
    "stats_table = stats_table[['useful', 'reviewerID']]\n",
    "stats_table.columns = ['useful','count']\n",
    "display(stats_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is obvious that useful reviews are the larger set with almost ~80% reviews. We remind the reader at this point that helpfulness has nothing to do with weather the sentiment of the review itself towards the book is either positive or negative. So, it is obvious at this point that we remove not useful reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Current number of reviews: \" + str(len(df6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7 = df6.loc[df6['useful'] == True]\n",
    "print(\"Current number of reviews: \" + str(len(df7)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep Books with a relatively high number of reviews\n",
    "Let's start with identifying the number of reviews per book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_per_book = pd.DataFrame(df7.groupby(['asin']).size())\n",
    "print(\"Number of books: \" + str(len(reviews_per_book)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_per_book = reviews_per_book.reset_index()\n",
    "reviews_per_book.columns = ['asin', 'number_of_reviews']\n",
    "reviews_per_book = reviews_per_book.sort_values(['number_of_reviews'], ascending=[False])\n",
    "reviews_per_book[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_per_book.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_per_book['number_of_reviews'].iplot(kind='histogram', bins=1000, xTitle='Number of Reviews', yTitle='Number of Books')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, it seems like, for the remaining reviews, `75%` of the books have 1-5 reviews. Assumming that 5 reviews or more are worth summarising, let's filter in only those with more than `5` reviews. At the same time it seems quite unreasonable to keep in the dataset books with an immense ammount of reviews (e.g. 1400). Arguably books with such numbers of reviews need to be summarised much more than others. However, reviews from these books will also pollute the overal corpus as they will concern very specific topic. Our objective is to maintain a balanced dataset with the numebr of reviews per book ranging within reasonable limits. Based on these and the distribution chart above, outliers (books with an overly high number of reviews) appear around 50-60 reviews. So we will set a threshold equal to 60 reviews max."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_per_book_gt = reviews_per_book.loc[reviews_per_book['number_of_reviews'] > 4]\n",
    "reviews_per_book_gt_lt = reviews_per_book_gt.loc[reviews_per_book_gt['number_of_reviews'] < 60]\n",
    "print(\"Number of books left: \" + str(len(reviews_per_book_gt_lt)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the distribution graph again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_per_book_gt_lt['number_of_reviews'].iplot(kind='histogram', bins=100, xTitle='Number of Reviews', yTitle='Number of Books')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in order to keep reviews for these books only we need to perform a JOIN operation on the original dataset with the following `books_to_keep` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_to_keep = pd.DataFrame(reviews_per_book_gt_lt['asin'])\n",
    "books_to_keep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current number of reviews\n",
    "print(\"Current number of reviews: \" + str(len(df7)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out reviews for books with less than 10 reviews and 200 or more reviews.\n",
    "df8 = pd.merge(df7, books_to_keep, on='asin', how='inner')\n",
    "print(\"Reviews with helpfullness ratings between [5,59]: \" + str(len(df8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df8.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving towards the normalisation section we need to get rig of unnecessary fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the columns necessary for the normalisation\n",
    "df9 = df8[['uniqueKey', 'reviewText']]\n",
    "df9.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step was necessary due to weird keyErrors that followed after trying to process the reviewText as a `pandas.DataFrame` and not as `pandas.Series`. After experimenting with both, I found that `pandas.Series.apply` is faster than `pandas.DataFrame.apply` and so I will hence work with `pandas.Series`. \n",
    "\n",
    "The assumption I require to make at this point before I follow is that `pandas` will not change the index of the reviews as those are being processed by my code and that in the end of my processing I will be able to re-associate those reviews with their **uniqueKey**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueKey_series_df = df9[['uniqueKey']]\n",
    "uniqueKey_series_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df = pd.DataFrame(df9['reviewText'].progress_apply(lambda review: review.split(\"\\n\")[0]))\n",
    "reviews_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Normalisation\n",
    "* Tokenization <span style=\"color:blue\"> DONE </span>\n",
    "* Convert All Tokens to Lowercase <span style=\"color:blue\"> DONE </span>\n",
    "* Eliminate Punctuation <span style=\"color:blue\"> DONE </span>\n",
    "* Remove Stop Words <span style=\"color:blue\"> DONE </span>\n",
    "* Changing Numbers into Words <span style=\"color:blue\"> DONE </span>\n",
    "* Expand Abbreviations <span style=\"color:red\"> NOT AS EASY AS I THOUGHT AND DOES NOT ADD MUCH VALUE</span> \n",
    "* Correct Spelling <span style=\"color:red\"> TOO SLOW (10h for 100k reviews)-->SO WONT DO</span>\n",
    "* Substituting Tokens with Synonyms <span style=\"color:green\"> TO DO</span>\n",
    "* Semantical Marking of Negatives <span style=\"color:blue\"> DONE (Ask Stasha's opinion) </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "tokenizer=RegexpTokenizer('[\\'\\w\\-]+',gaps=False)\n",
    "\n",
    "step_0_df = reviews_df['reviewText'].progress_apply(lambda review: tokenizer.tokenize(review))\n",
    "step_0_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Tokens to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def convert_to_lowercase(review):\n",
    "\n",
    "    for i in range(len(review)):\n",
    "        review[i] = review[i].lower()\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_1_df = step_0_df.progress_apply(lambda review: convert_to_lowercase(review))\n",
    "step_1_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminate Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def eliminate_punctuation(review, regex):\n",
    "    new_review = []\n",
    "    for token in review:\n",
    "        new_token = regex.sub(u'', token)\n",
    "        if not new_token == u'':\n",
    "            new_review.append(new_token)\n",
    "    return new_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex=re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "step_2_df = step_1_df.progress_apply(lambda review: eliminate_punctuation(review, regex))\n",
    "step_2_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing Numbers into Words\n",
    "It is a common case to encounter numbers attached to words like:\n",
    "\n",
    "```\n",
    "21st\n",
    "1980oct\n",
    "```\n",
    "\n",
    "This may be due to mistakes or ofr other reasons. What we care to do is to split words from numbers and add numbers as separate tokens in each review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "r1 = re.compile(\"([a-zA-Z]+)([0-9]+)\")\n",
    "r2 = re.compile(\"([0-9]+)([a-zA-Z]+)\")\n",
    "r3 = re.compile(\"([a-zA-Z]+)([0-9]+)([a-zA-Z]+)\")\n",
    "r4 = re.compile(\"([0-9]+)([a-zA-Z]+)([0-9]+)\")\n",
    "\n",
    "def split_words_and_nums(review):\n",
    "    new_review = []\n",
    "    for token in review:\n",
    "        firstRegexIsTrue = r1.match(token)\n",
    "        secondRegexIsTrue = r2.match(token)\n",
    "        thirdRegexIsTrue = r3.match(token)\n",
    "        fourthRegexIsTrue = r4.match(token)\n",
    "    \n",
    "        if(firstRegexIsTrue):\n",
    "            new_review.append(firstRegexIsTrue.group(0))\n",
    "            new_review.append(firstRegexIsTrue.group(1))\n",
    "        elif(firstRegexIsTrue):\n",
    "            new_review.append(secondRegexIsTrue.group(0))\n",
    "            new_review.append(secondRegexIsTrue.group(1))\n",
    "        elif(thirdRegexIsTrue):\n",
    "            new_review.append(thirdRegexIsTrue.group(0))\n",
    "            new_review.append(thirdRegexIsTrue.group(1))\n",
    "            new_review.append(thirdRegexIsTrue.group(2))\n",
    "        elif(fourthRegexIsTrue):\n",
    "            new_review.append(fourthRegexIsTrue.group(0))\n",
    "            new_review.append(fourthRegexIsTrue.group(1))\n",
    "            new_review.append(fourthRegexIsTrue.group(2))\n",
    "        else:\n",
    "            new_review.append(token)\n",
    "    return new_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_3_df = step_2_df.progress_apply(lambda review: split_words_and_nums(review))\n",
    "step_3_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, before converting words into numbers, we should account for the fact that very big numbers mmay accidentally be palced into our reviews. Therefore, we will only consider converting numbers that are no more than 10 digits long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import inflect\n",
    "p = inflect.engine()\n",
    "\n",
    "def numStringToWord(review, p):        \n",
    "    for i in range(len(review)):\n",
    "        if(review[i].isdigit()):\n",
    "            if(len(review[i])<10):\n",
    "                review[i] = p.number_to_words(review[i])\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_4_df = step_3_df.progress_apply(lambda review: numStringToWord(review, p))\n",
    "step_4_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct Spelling\n",
    "It turns out that this is a very expensive operation to run at this stage. In addition to this, the solution below by which I am able to substitue synonyms, also works as a spellchecker and hence applying a spell-checker directly and at this point is both non-efficient and seems to also be redundant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# from autocorrect import spell\n",
    "\n",
    "# def spellCheck(review):\n",
    "\n",
    "#     for i in range(len(review)):\n",
    "#         review[i] = spell(review[i])\n",
    "#     return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# step_5_df = step_4_df.progress_apply(lambda review: spellCheck(review))\n",
    "# step_5_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Substituting Tokens with Synonyms\n",
    "Replacing words with synonyms is a very tricky operation. In fact,synonyms are a huge and open area of work in natural language processing.\n",
    "\n",
    "The problem is multifaceted and mostly derives from the fact that for any single word, and especially for adjectives, there is no one single adjective to replace it with. Suppose for instance that we rely on a solution based on the `PyDictionary` library. In the below example, you can clearly see that there are multiple words to be chosen for a certain noun: \n",
    "\n",
    "```python\n",
    ">>> from PyDictionary import PyDictionary\n",
    "\n",
    ">>> dictionary=PyDictionary()\n",
    "print (dictionary.synonym(\"Life\"))\n",
    "['heart', 'growth', 'soul', 'activity', 'get-up-and-go']\n",
    "```\n",
    "It is therefore necessary that one defines a way by which a certain alternative is chosen from the resulting list. In `NLP` a list of synonyms is commonly reffered to as `synset`. \n",
    "\n",
    "A naive way to go about solving this problem is to choose the most common member in the set. The `nltk` will let you build a frequency table in just a few lines of code ([link](https://stackoverflow.com/questions/38233145/nltk-most-common-synonym-wordnet-for-each-word)). \n",
    "```python\n",
    ">>> from nltk.corpus import brown\n",
    ">>> freqs = nltk.FreqDist(w.lower() for w in brown.words())\n",
    ">>> print(freqs[\"valued\"])\n",
    "14\n",
    "```\n",
    "But, in turn, one has to also figure out how to define the \"most common member\" in the set. This could be based on measuring the each members appearance frequence in the whole corpus. However, this would also be problematic for number of reasons; the main one relates to the semantical meaning of synonyms.\n",
    "\n",
    "Synomyms are not semantically equal words in terms of their meaning. For instance, in the previous example, \"growth\" appears  to be a synonym for \"life\", yet, though related, the two words have different interpretations. Furthermore, if we take each word as a signleton and try to interpret its meaning irrespective of the context it is being used in, it is quite possible that our interpretation will deviade from the words intended meaning. The general point is that the context in which a word is used is also important. If we rely on the whole corpus to define the importance of a word over others then we may end up replacing words in book reviews, not based on how often those words are used with respect to the book at hand but with respect to the whole corpus of reviews for all kinds of books. \n",
    "\n",
    "A more sophisticated approach would focus on identifying word frequencies within the scope of reviews for a certain book. For that, one would need to:\n",
    "\n",
    "1. Construct a set of review corpuses, one for every book;\n",
    "2. Create a dictionary of frequencies of words for each of these corpuses, and;\n",
    "3. Use the dictionary to choose the word with the highest frequency in a synset.\n",
    "\n",
    "In addition to the above problem, one needs to tackle another difficulty. The point of replacing sysonyms is to reduce the vocabulary of a text. By compressing the vocabulary without losing meaning, you can save memory in cases such as frequency analysis and text indexing (https://en.wikipedia.org/wiki/Frequency_analysis). Vocabulary reduction can also increase the occurrence of significant collocations. But this can only be achieved as long a the same synonym is consistently chosen from a `synset` and the semantical meaning of the word it replaces is not distorted. Let us illustrate why this is a complex task. Following on the previous example:\n",
    "```python\n",
    ">>> print (dictionary.synonym(\"growth\"))\n",
    "['prosperity', 'success', 'advance', 'hike', 'rise']\n",
    ">>> print (dictionary.synonym(\"success\"))\n",
    "['prosperity', 'advance', 'achievement', 'win', 'accomplishment']\n",
    ">>> print (dictionary.synonym(\"prosperity\"))\n",
    "['wealth', 'success', 'accomplishment', 'riches', 'expansion']\n",
    ">>> print (dictionary.synonym(\"advance\"))\n",
    "['forward', 'leading', 'prior', 'first', 'beforehand']\n",
    "```\n",
    "\n",
    "Notice, that \"growth\" and \"success\" don't share the exact same `synset`, so it is possible that while one synnonym is chosen for the first, another is chosen for the second. This incosistency will reduce the chances of minimising the corpus' vocabulary. Furhtermore, suppose that it just so happens that both words are replaced with one of the commonly shared synonyms--that is either 'prosperity' or 'advance'--then, if it is propserity, then propserity should not be replaced when encountered at all!\n",
    "\n",
    "This problem, though complicated, can be handle with the developement of a well-thought algorithm. However, if the objective is to replace each and every word in the dataset, it will still fail as it is very likely that the solution will not be efficient or, worse, tractable. At the same time, it is quite possible that it will distort the text it originated from. To counter these problems, one can only apply this solution only on certain kinds of words, and specifically on adjectives. This approach offers two  significant advantages:\n",
    "\n",
    "1. It reduces the application scope to something more tractable, and;\n",
    "2. Minimises the risk of distorting meaning as adjectives and their `synsets` are more strictly defined and their meaning is unlikely to significantly deviate from the original word.\n",
    "\n",
    "The above points can be justified simply by looking at the synsets of two random synonyms: \n",
    "```python\n",
    ">>> print (dictionary.synonym(\"hard\"))\n",
    "['solid', 'strong', 'tough', 'concentrated', 'callous']\n",
    ">>> print (dictionary.synonym(\"tough\"))\n",
    "['tenacious', 'vigorous', 'stiff', 'solid', 'hard']\n",
    "```\n",
    "\n",
    "As such, replacing of synonyms will be postponed until after POS-tagging is applied in the next section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing Negations with Antonyms\n",
    "The opposite of synonym replacement is antonym replacement. An antonym is a word that has the opposite meaning of another word. This time, instead of creating custom word mappings, we can use WordNet to replace words with unambiguous antonyms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class AntonymReplacer(object):\n",
    "    def replace(self, token, pos=None):\n",
    "        antonyms = set()\n",
    "        for syn in wordnet.synsets(token, pos=pos):\n",
    "            for lemma in syn.lemmas():\n",
    "                for antonym in lemma.antonyms():\n",
    "                    antonyms.add(antonym.name())\n",
    "        if len(antonyms) == 1:\n",
    "            return antonyms.pop()\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def replace_negations(self, review):\n",
    "        i, l = 0, len(review)\n",
    "        tokens = []\n",
    "        while i<l:\n",
    "            token = review[i]\n",
    "            if token == 'not' and i+1 <l:\n",
    "                ant = self.replace(review[i+1])\n",
    "                if ant:\n",
    "                    tokens.append(ant)\n",
    "                    i += 2\n",
    "                    continue\n",
    "            tokens.append(token)\n",
    "            i += 1\n",
    "\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Lets see an example\n",
    "replacer = AntonymReplacer()\n",
    "replacer.replace(\"good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacer.replace(\"uglify\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "review = [\"lets\",\"not\",\"uglify\",\"our\",\"code\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacer.replace_negations(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_5_df = step_4_df.progress_apply(lambda review: replacer.replace_negations(review))\n",
    "step_5_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### Remove Stop Words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(review):\n",
    "    return [token for token in review if not token in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_6_df = step_5_df.progress_apply(lambda review: remove_stopwords(review))\n",
    "step_6_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JOIN Reviews with their Original Keys & Filter Out Empty Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert output to Dataframe\n",
    "step_6_df = pd.DataFrame(step_6_df)\n",
    "len(step_6_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JOIN normalised reviews with their original keys\n",
    "tokenized_keyed_reviews = pd.concat([uniqueKey_series_df, step_6_df], axis=1);\n",
    "tokenized_keyed_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove empty reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_keyed_reviews = tokenized_keyed_reviews[(tokenized_keyed_reviews['reviewText'].str.len() != 0) | (tokenized_keyed_reviews['reviewText'].str.len() != 0)]\n",
    "len(tokenized_keyed_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## Persist DF to avoid Re-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tokenized_keyed_reviews.to_csv(\"../data/interim/001_normalised_keyed_reviews.csv\", sep='\\t', header=True, index=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tokenized_keyed_reviews[0:99999].to_csv(\"../data/interim/001_normalised_keyed_reviews_100k_sample.csv\", sep='\\t', header=True, index=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tokenized_keyed_reviews[0:99].to_csv(\"../data/interim/001_normalised_keyed_reviews_100_rows_sample.csv\", sep='\\t', header=True, index=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## END_OF_FILE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
