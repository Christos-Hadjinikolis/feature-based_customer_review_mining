{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chadjinik/opt/anaconda3/lib/python3.7/site-packages/tqdm/std.py:648: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "# For monitoring duration of pandas processes\n",
    "from tqdm import tqdm, tqdm_pandas\n",
    "\n",
    "# To avoid RuntimeError: Set changed size during iteration\n",
    "tqdm.monitor_interval = 0\n",
    "\n",
    "# Register `pandas.progress_apply` and `pandas.Series.map_apply` with `tqdm`\n",
    "# (can use `tqdm_gui`, `tqdm_notebook`, optional kwargs, etc.)\n",
    "tqdm.pandas(desc=\"Progress:\")\n",
    "\n",
    "# Now you can use `progress_apply` instead of `apply`\n",
    "# and `progress_map` instead of `map`\n",
    "# can also groupby:\n",
    "# df.groupby(0).progress_apply(lambda x: x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df0 = pd.read_pickle('../data/interim/004_synonyms_grouped_1k.p')\n",
    "df0 = pd.read_pickle('../data/interim/002_keyed_nouns.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniqueKey</th>\n",
       "      <th>reviewText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>A2XQ5LZHTD4AFT##000100039X</td>\n",
       "      <td>[timeless,  gibran,  backs,  content,  means, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>AF7CSSGV93RXN##000100039X</td>\n",
       "      <td>[ prophet,  kahlil,  gibran,  thirty,  years, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>A1NPNGWBVD9AK3##000100039X</td>\n",
       "      <td>[ first,  books,  recall,  collection,  gibran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>A3IS4WGMFR4X65##000100039X</td>\n",
       "      <td>[prophet,  kahlil,  work,  world,  million,  c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>AWLFVCT9128JV##000100039X</td>\n",
       "      <td>[gibran,  khalil,  gibran,  born,  one thousan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    uniqueKey  \\\n",
       "0  A2XQ5LZHTD4AFT##000100039X   \n",
       "1   AF7CSSGV93RXN##000100039X   \n",
       "2  A1NPNGWBVD9AK3##000100039X   \n",
       "3  A3IS4WGMFR4X65##000100039X   \n",
       "4   AWLFVCT9128JV##000100039X   \n",
       "\n",
       "                                          reviewText  \n",
       "0  [timeless,  gibran,  backs,  content,  means, ...  \n",
       "1  [ prophet,  kahlil,  gibran,  thirty,  years, ...  \n",
       "2  [ first,  books,  recall,  collection,  gibran...  \n",
       "3  [prophet,  kahlil,  work,  world,  million,  c...  \n",
       "4  [gibran,  khalil,  gibran,  born,  one thousan...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_df00 = pd.read_pickle('../data/interim/003_dictionary.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "810003"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary_df00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>book</td>\n",
       "      <td>1503414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>one</td>\n",
       "      <td>660626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>read</td>\n",
       "      <td>484457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>like</td>\n",
       "      <td>403304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>story</td>\n",
       "      <td>366204</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     word  frequency\n",
       "0    book    1503414\n",
       "1     one     660626\n",
       "2    read     484457\n",
       "3    like     403304\n",
       "4   story     366204"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary_df00.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The idea\n",
    "Words that only appear once cannot be frequent words even in their own context; so they will be filtered out. Then lets calculate the average frequency for the remaining words--remember; this dictionary does not only concern nouns.\n",
    "\n",
    "<span style=\"color:red\"> Notice: grouping of noun synonyms done in `004_grouping_domain_synonyms` is repeated here once filtering out nouns is applied, since it will take far less time to be applied on the whole dataset once the latter is filter (`004_grouping_domain_synonyms` was aplied only on 1k reviews)  </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>1.508030e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>5.528967e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>6.719492e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>6.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>1.000000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>2.200000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>9.200000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>1.503414e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          frequency\n",
       "count  1.508030e+05\n",
       "mean   5.528967e+02\n",
       "std    6.719492e+03\n",
       "min    6.000000e+00\n",
       "25%    1.000000e+01\n",
       "50%    2.200000e+01\n",
       "75%    9.200000e+01\n",
       "max    1.503414e+06"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary_df00.loc[dictionary_df00['frequency'] > 5].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "167645"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary_df00['word'].loc[dictionary_df00['frequency'] > 4].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt4_dictionary_df01 = dictionary_df00.loc[dictionary_df00['frequency'] > 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1.676450e+05\n",
       "mean     4.978538e+02\n",
       "std      6.375158e+03\n",
       "min      5.000000e+00\n",
       "25%      8.000000e+00\n",
       "50%      1.800000e+01\n",
       "75%      7.500000e+01\n",
       "max      1.503414e+06\n",
       "Name: frequency, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary_df00['frequency'].loc[dictionary_df00['frequency'] > 4].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38874"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use threshold for first quantile\n",
    "final_dic = gt4_dictionary_df01.loc[dictionary_df00['frequency'] < 8]\n",
    "len(final_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:: 100%|██████████| 38874/38874 [00:00<00:00, 1148518.11it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>frequency</th>\n",
       "      <th>normalised</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>128771</td>\n",
       "      <td>candids</td>\n",
       "      <td>7</td>\n",
       "      <td>0.014403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128772</td>\n",
       "      <td>sapas</td>\n",
       "      <td>7</td>\n",
       "      <td>0.014403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128773</td>\n",
       "      <td>wayit</td>\n",
       "      <td>7</td>\n",
       "      <td>0.014403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128774</td>\n",
       "      <td>shamen</td>\n",
       "      <td>7</td>\n",
       "      <td>0.014403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128775</td>\n",
       "      <td>arnita</td>\n",
       "      <td>7</td>\n",
       "      <td>0.014403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            word  frequency  normalised\n",
       "128771   candids          7    0.014403\n",
       "128772     sapas          7    0.014403\n",
       "128773     wayit          7    0.014403\n",
       "128774    shamen          7    0.014403\n",
       "128775    arnita          7    0.014403"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dic_df01 = final_dic.assign(normalised = final_dic['frequency'].progress_apply(lambda frequency:frequency/486))\n",
    "final_dic_df01.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begin noun filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniqueKey</th>\n",
       "      <th>reviewText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>A2XQ5LZHTD4AFT##000100039X</td>\n",
       "      <td>[timeless,  gibran,  backs,  content,  means, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>AF7CSSGV93RXN##000100039X</td>\n",
       "      <td>[ prophet,  kahlil,  gibran,  thirty,  years, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>A1NPNGWBVD9AK3##000100039X</td>\n",
       "      <td>[ first,  books,  recall,  collection,  gibran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>A3IS4WGMFR4X65##000100039X</td>\n",
       "      <td>[prophet,  kahlil,  work,  world,  million,  c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>AWLFVCT9128JV##000100039X</td>\n",
       "      <td>[gibran,  khalil,  gibran,  born,  one thousan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    uniqueKey  \\\n",
       "0  A2XQ5LZHTD4AFT##000100039X   \n",
       "1   AF7CSSGV93RXN##000100039X   \n",
       "2  A1NPNGWBVD9AK3##000100039X   \n",
       "3  A3IS4WGMFR4X65##000100039X   \n",
       "4   AWLFVCT9128JV##000100039X   \n",
       "\n",
       "                                          reviewText  \n",
       "0  [timeless,  gibran,  backs,  content,  means, ...  \n",
       "1  [ prophet,  kahlil,  gibran,  thirty,  years, ...  \n",
       "2  [ first,  books,  recall,  collection,  gibran...  \n",
       "3  [prophet,  kahlil,  work,  world,  million,  c...  \n",
       "4  [gibran,  khalil,  gibran,  born,  one thousan...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>asin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>A2XQ5LZHTD4AFT</td>\n",
       "      <td>000100039X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>AF7CSSGV93RXN</td>\n",
       "      <td>000100039X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>A1NPNGWBVD9AK3</td>\n",
       "      <td>000100039X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>A3IS4WGMFR4X65</td>\n",
       "      <td>000100039X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>AWLFVCT9128JV</td>\n",
       "      <td>000100039X</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           userId        asin\n",
       "0  A2XQ5LZHTD4AFT  000100039X\n",
       "1   AF7CSSGV93RXN  000100039X\n",
       "2  A1NPNGWBVD9AK3  000100039X\n",
       "3  A3IS4WGMFR4X65  000100039X\n",
       "4   AWLFVCT9128JV  000100039X"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.DataFrame(df0.uniqueKey.str.split('##',1).tolist(),columns = ['userId','asin'])\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>[timeless,  gibran,  backs,  content,  means, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>[ prophet,  kahlil,  gibran,  thirty,  years, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>[ first,  books,  recall,  collection,  gibran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>[prophet,  kahlil,  work,  world,  million,  c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>[gibran,  khalil,  gibran,  born,  one thousan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          reviewText\n",
       "0  [timeless,  gibran,  backs,  content,  means, ...\n",
       "1  [ prophet,  kahlil,  gibran,  thirty,  years, ...\n",
       "2  [ first,  books,  recall,  collection,  gibran...\n",
       "3  [prophet,  kahlil,  work,  world,  million,  c...\n",
       "4  [gibran,  khalil,  gibran,  born,  one thousan..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reviewText = pd.DataFrame(df0['reviewText'])\n",
    "df_reviewText.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>A2XQ5LZHTD4AFT</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>[timeless,  gibran,  backs,  content,  means, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>AF7CSSGV93RXN</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>[ prophet,  kahlil,  gibran,  thirty,  years, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>A1NPNGWBVD9AK3</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>[ first,  books,  recall,  collection,  gibran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>A3IS4WGMFR4X65</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>[prophet,  kahlil,  work,  world,  million,  c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>AWLFVCT9128JV</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>[gibran,  khalil,  gibran,  born,  one thousan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           userId        asin  \\\n",
       "0  A2XQ5LZHTD4AFT  000100039X   \n",
       "1   AF7CSSGV93RXN  000100039X   \n",
       "2  A1NPNGWBVD9AK3  000100039X   \n",
       "3  A3IS4WGMFR4X65  000100039X   \n",
       "4   AWLFVCT9128JV  000100039X   \n",
       "\n",
       "                                          reviewText  \n",
       "0  [timeless,  gibran,  backs,  content,  means, ...  \n",
       "1  [ prophet,  kahlil,  gibran,  thirty,  years, ...  \n",
       "2  [ first,  books,  recall,  collection,  gibran...  \n",
       "3  [prophet,  kahlil,  work,  world,  million,  c...  \n",
       "4  [gibran,  khalil,  gibran,  born,  one thousan...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new = pd.concat([df1, df_reviewText], axis=1)\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:: 100%|██████████| 582711/582711 [00:00<00:00, 1095439.12it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>wordCountBefore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>A2XQ5LZHTD4AFT</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>[timeless,  gibran,  backs,  content,  means, ...</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>AF7CSSGV93RXN</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>[ prophet,  kahlil,  gibran,  thirty,  years, ...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>A1NPNGWBVD9AK3</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>[ first,  books,  recall,  collection,  gibran...</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>A3IS4WGMFR4X65</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>[prophet,  kahlil,  work,  world,  million,  c...</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>AWLFVCT9128JV</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>[gibran,  khalil,  gibran,  born,  one thousan...</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           userId        asin  \\\n",
       "0  A2XQ5LZHTD4AFT  000100039X   \n",
       "1   AF7CSSGV93RXN  000100039X   \n",
       "2  A1NPNGWBVD9AK3  000100039X   \n",
       "3  A3IS4WGMFR4X65  000100039X   \n",
       "4   AWLFVCT9128JV  000100039X   \n",
       "\n",
       "                                          reviewText  wordCountBefore  \n",
       "0  [timeless,  gibran,  backs,  content,  means, ...               49  \n",
       "1  [ prophet,  kahlil,  gibran,  thirty,  years, ...               19  \n",
       "2  [ first,  books,  recall,  collection,  gibran...               74  \n",
       "3  [prophet,  kahlil,  work,  world,  million,  c...              142  \n",
       "4  [gibran,  khalil,  gibran,  born,  one thousan...               48  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new_01 = df_new.assign(wordCountBefore = df_new['reviewText'].progress_apply(lambda review:len(review)))\n",
    "df_new_01.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:: 100%|██████████| 38874/38874 [00:00<00:00, 784366.36it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>word</th>\n",
       "      <th>frequency</th>\n",
       "      <th>normalised</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>128771</td>\n",
       "      <td>candids</td>\n",
       "      <td>7</td>\n",
       "      <td>0.014403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>128772</td>\n",
       "      <td>sapas</td>\n",
       "      <td>7</td>\n",
       "      <td>0.014403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>128773</td>\n",
       "      <td>wayit</td>\n",
       "      <td>7</td>\n",
       "      <td>0.014403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>128774</td>\n",
       "      <td>shamen</td>\n",
       "      <td>7</td>\n",
       "      <td>0.014403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>128775</td>\n",
       "      <td>arnita</td>\n",
       "      <td>7</td>\n",
       "      <td>0.014403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index     word  frequency  normalised\n",
       "0  128771  candids          7    0.014403\n",
       "1  128772    sapas          7    0.014403\n",
       "2  128773    wayit          7    0.014403\n",
       "3  128774   shamen          7    0.014403\n",
       "4  128775   arnita          7    0.014403"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dic_df01['word'] = final_dic_df01['word'].progress_apply(lambda word: word.replace(\" \",\"\"))\n",
    "final_dic_df01 = final_dic_df01.reset_index()\n",
    "final_dic_df01.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'candids': 0,\n",
       " 'sapas': 1,\n",
       " 'wayit': 2,\n",
       " 'shamen': 3,\n",
       " 'arnita': 4,\n",
       " 'gazzy': 5,\n",
       " 'faltha': 6,\n",
       " 'charcterization': 7,\n",
       " 'sevenbook': 8,\n",
       " 'moomintroll': 9,\n",
       " 'onionlike': 10,\n",
       " 'schars': 11,\n",
       " 'polemicism': 12,\n",
       " 'brevet': 13,\n",
       " 'nakamuras': 14,\n",
       " 'maximum': 15,\n",
       " 'riviere': 16,\n",
       " 'caricatural': 17,\n",
       " 'hornbook': 18,\n",
       " 'romanum': 19,\n",
       " 'storyby': 20,\n",
       " 'oopsie': 21,\n",
       " 'singletitle': 22,\n",
       " 'brawns': 23,\n",
       " 'chatacter': 24,\n",
       " 'awoman': 25,\n",
       " 'suzy': 26,\n",
       " 'exwrestler': 27,\n",
       " 'safty': 28,\n",
       " 'rossums': 29,\n",
       " 'applequist': 30,\n",
       " 'deatri': 31,\n",
       " 'zerek': 32,\n",
       " 'dirmann': 33,\n",
       " 'alongand': 34,\n",
       " 'teenyboppers': 35,\n",
       " 'coban': 36,\n",
       " 'manchester': 37,\n",
       " 'brent': 38,\n",
       " 'sedge': 39,\n",
       " 'persoanlly': 40,\n",
       " '7mm': 41,\n",
       " 'futureworld': 42,\n",
       " 'glimpses': 43,\n",
       " 'sharrow': 44,\n",
       " 'lorkin': 45,\n",
       " 'sachaka': 46,\n",
       " 'lissys': 47,\n",
       " 'malinda': 48,\n",
       " 'websurfing': 49,\n",
       " 'etceteras': 50,\n",
       " 'mindanao': 51,\n",
       " 'souvlaki': 52,\n",
       " 'madlibs': 53,\n",
       " 'darkwing': 54,\n",
       " 'allbeit': 55,\n",
       " 'vayl': 56,\n",
       " 'lessee': 57,\n",
       " 'polchinski': 58,\n",
       " 'wellproportioned': 59,\n",
       " 'highers': 60,\n",
       " 'wackedout': 61,\n",
       " 'tauntingly': 62,\n",
       " 'eightyearold': 63,\n",
       " 'mislabled': 64,\n",
       " 'flore': 65,\n",
       " 'tibbetts': 66,\n",
       " 'niederhoffers': 67,\n",
       " 'rhodesians': 68,\n",
       " 'romace': 69,\n",
       " 'dejonge': 70,\n",
       " 'devens': 71,\n",
       " 'merion': 72,\n",
       " 'maloof': 73,\n",
       " 'intself': 74,\n",
       " 'mixon': 75,\n",
       " 'zhong': 76,\n",
       " 'ahtan': 77,\n",
       " 'auhor': 78,\n",
       " 'niccis': 79,\n",
       " 'zedds': 80,\n",
       " 'fivethousand': 81,\n",
       " 'extralong': 82,\n",
       " 'capitivity': 83,\n",
       " 'cthulhus': 84,\n",
       " 'allohistorical': 85,\n",
       " 'category': 86,\n",
       " 'aone': 87,\n",
       " 'tirith': 88,\n",
       " 'lornth': 89,\n",
       " 'coring': 90,\n",
       " 'katos': 91,\n",
       " 'existence': 92,\n",
       " 'miniquest': 93,\n",
       " 'talkingheads': 94,\n",
       " 'padraic': 95,\n",
       " 'anette': 96,\n",
       " 'yourselfi': 97,\n",
       " 'macdermott': 98,\n",
       " 'footrace': 99,\n",
       " 'massenergy': 100,\n",
       " 'hartwells': 101,\n",
       " 'foreverthe': 102,\n",
       " 'moonheart': 103,\n",
       " 'yesod': 104,\n",
       " 'luczaks': 105,\n",
       " 'aimes': 106,\n",
       " 'honoras': 107,\n",
       " 'dreys': 108,\n",
       " 'nonphilosophical': 109,\n",
       " 'rhialto': 110,\n",
       " 'fictionfantasy': 111,\n",
       " 'mikalya': 112,\n",
       " 'cuhulain': 113,\n",
       " 'dangereuse': 114,\n",
       " 'realeased': 115,\n",
       " 'weretigers': 116,\n",
       " 'fomorii': 117,\n",
       " 'storiesi': 118,\n",
       " 'robberyhomicide': 119,\n",
       " 'nonbosch': 120,\n",
       " 'surpises': 121,\n",
       " 'sereis': 122,\n",
       " 'skotos': 123,\n",
       " 'nykyrian': 124,\n",
       " 'valchek': 125,\n",
       " 'cubbies': 126,\n",
       " 'kittredges': 127,\n",
       " 'gratutitous': 128,\n",
       " 'captiva': 129,\n",
       " 'progenies': 130,\n",
       " 'lowfrequency': 131,\n",
       " 'gulo': 132,\n",
       " 'longcherished': 133,\n",
       " 'avar': 134,\n",
       " 'valedon': 135,\n",
       " 'valans': 136,\n",
       " 'parthenogenesis': 137,\n",
       " 'bobbit': 138,\n",
       " 'adikor': 139,\n",
       " 'qirsi': 140,\n",
       " 'goblinkin': 141,\n",
       " 'sainnites': 142,\n",
       " 'pellaz': 143,\n",
       " 'homophone': 144,\n",
       " 'eidolons': 145,\n",
       " 'agiel': 146,\n",
       " 'horselike': 147,\n",
       " 'scimina': 148,\n",
       " 'vulcanears': 149,\n",
       " 'magicusing': 150,\n",
       " 'fredas': 151,\n",
       " 'ambros': 152,\n",
       " 'nazification': 153,\n",
       " 'phenol': 154,\n",
       " 'bakeshop': 155,\n",
       " 'peterhof': 156,\n",
       " 'toti': 157,\n",
       " 'summaryi': 158,\n",
       " 'indead': 159,\n",
       " 'eople': 160,\n",
       " 'nyberg': 161,\n",
       " 'kildall': 162,\n",
       " 'chronological': 163,\n",
       " 'passionand': 164,\n",
       " '500odd': 165,\n",
       " 'awefull': 166,\n",
       " 'hawk': 167,\n",
       " 'pob': 168,\n",
       " 'cst': 169,\n",
       " 'sacculina': 170,\n",
       " 'coochie': 171,\n",
       " 'thatthese': 172,\n",
       " 'mitchellcoauthor': 173,\n",
       " 'sofrep': 174,\n",
       " 'bloodynine': 175,\n",
       " 'glotka': 176,\n",
       " 'sixshooters': 177,\n",
       " 'rothfusss': 178,\n",
       " 'duelists': 179,\n",
       " 'gunwielding': 180,\n",
       " 'gabriella': 181,\n",
       " 'ign': 182,\n",
       " 'freireich': 183,\n",
       " 'thatnot': 184,\n",
       " 'tandys': 185,\n",
       " 'inoculating': 186,\n",
       " 'prevaricating': 187,\n",
       " 'heggan': 188,\n",
       " 'atsugi': 189,\n",
       " 'rooseveltian': 190,\n",
       " 'selfsoothe': 191,\n",
       " 'stayathomemom': 192,\n",
       " 'mambypamby': 193,\n",
       " 'hardigree': 194,\n",
       " 'housebuilding': 195,\n",
       " 'alleven': 196,\n",
       " 'cursed': 197,\n",
       " 'hushs': 198,\n",
       " 'candacekof': 199,\n",
       " 'selfsearching': 200,\n",
       " 'timethere': 201,\n",
       " 'carded': 202,\n",
       " 'tieger': 203,\n",
       " 'enfp': 204,\n",
       " 'hantavirus': 205,\n",
       " 'depends': 206,\n",
       " 'selfpublishes': 207,\n",
       " 'vhari': 208,\n",
       " 'thatmost': 209,\n",
       " 'bermondsey': 210,\n",
       " 'schulzes': 211,\n",
       " 'troubleand': 212,\n",
       " 'sandbagged': 213,\n",
       " 'freedomand': 214,\n",
       " 'stymies': 215,\n",
       " 'nonsexist': 216,\n",
       " 'sameday': 217,\n",
       " 'mindshattering': 218,\n",
       " 'overobvious': 219,\n",
       " 'keidis': 220,\n",
       " 'laydown': 221,\n",
       " 'streetsmarts': 222,\n",
       " 'obeysekere': 223,\n",
       " 'aj': 224,\n",
       " 'psywar': 225,\n",
       " 'jemison': 226,\n",
       " 'knowed': 227,\n",
       " 'paint': 228,\n",
       " 'exilic': 229,\n",
       " 'prophecied': 230,\n",
       " 'itex': 231,\n",
       " 'downum': 232,\n",
       " 'specifially': 233,\n",
       " 'rawlinss': 234,\n",
       " 'hokitika': 235,\n",
       " 'mannering': 236,\n",
       " 'ripped': 237,\n",
       " 'ethologists': 238,\n",
       " 'spiritwalker': 239,\n",
       " 'zombieish': 240,\n",
       " 'firebomb': 241,\n",
       " 'energyintensive': 242,\n",
       " 'leaford': 243,\n",
       " 'farhad': 244,\n",
       " 'africanized': 245,\n",
       " 'amirrezvani': 246,\n",
       " 'godlings': 247,\n",
       " 'girdlegard': 248,\n",
       " 'prolyl': 249,\n",
       " 'phenyl': 250,\n",
       " 'glutaminyl': 251,\n",
       " 'arginyl': 252,\n",
       " 'gallium': 253,\n",
       " 'poptart': 254,\n",
       " 'korytas': 255,\n",
       " 'nailers': 256,\n",
       " 'noninteresting': 257,\n",
       " 'ocho': 258,\n",
       " 'alexia': 259,\n",
       " 'newlycreated': 260,\n",
       " 'reddening': 261,\n",
       " '5plot': 262,\n",
       " 'expell': 263,\n",
       " 'waksal': 264,\n",
       " 'subtitute': 265,\n",
       " 'cookbookthe': 266,\n",
       " 'ciasponsored': 267,\n",
       " 'stelian': 268,\n",
       " '5writing': 269,\n",
       " 'belowi': 270,\n",
       " 'crowbars': 271,\n",
       " 'jaywalkers': 272,\n",
       " 'exlaw': 273,\n",
       " 'voldemorts': 274,\n",
       " 'cants': 275,\n",
       " 'tigres': 276,\n",
       " 'gbs': 277,\n",
       " 'narrows': 278,\n",
       " 'feiges': 279,\n",
       " 'indichova': 280,\n",
       " 'cocaptain': 281,\n",
       " '1chapter': 282,\n",
       " 'optimisim': 283,\n",
       " 'abrahamson': 284,\n",
       " 'powerhouse': 285,\n",
       " 'zowie': 286,\n",
       " 'grouches': 287,\n",
       " 'loupe': 288,\n",
       " 'myfanwys': 289,\n",
       " 'myfawny': 290,\n",
       " 'ralpha': 291,\n",
       " 'explenation': 292,\n",
       " 'ochres': 293,\n",
       " 'adytum': 294,\n",
       " 'percipience': 295,\n",
       " 'corgis': 296,\n",
       " 'onesies': 297,\n",
       " 'cakechocolate': 298,\n",
       " 'spark': 299,\n",
       " 'viron': 300,\n",
       " 'boppers': 301,\n",
       " 'cauthon': 302,\n",
       " 'voilent': 303,\n",
       " 'ankas': 304,\n",
       " 'colostomy': 305,\n",
       " 'labrynthine': 306,\n",
       " 'divagations': 307,\n",
       " 'joana': 308,\n",
       " 'greenberger': 309,\n",
       " 'roxelana': 310,\n",
       " 'wukong': 311,\n",
       " 'partnersincrime': 312,\n",
       " 'maudies': 313,\n",
       " 'cach': 314,\n",
       " 'entrepeneurial': 315,\n",
       " 'cabral': 316,\n",
       " 'flatworms': 317,\n",
       " 'tweetie': 318,\n",
       " 'sunee': 319,\n",
       " 'kramisha': 320,\n",
       " 'disorients': 321,\n",
       " 'camisoles': 322,\n",
       " 'reasonby': 323,\n",
       " 'jacis': 324,\n",
       " 'moony': 325,\n",
       " 'threephase': 326,\n",
       " 'agatson': 327,\n",
       " 'sirk': 328,\n",
       " 'stitch': 329,\n",
       " '14years': 330,\n",
       " 'licketysplit': 331,\n",
       " 'tromped': 332,\n",
       " 'ishigamis': 333,\n",
       " 'congressionally': 334,\n",
       " 'surette': 335,\n",
       " 'speeddial': 336,\n",
       " 'weathy': 337,\n",
       " 'genrebut': 338,\n",
       " 'oboist': 339,\n",
       " 'islamicized': 340,\n",
       " 'sciascias': 341,\n",
       " 'biopsychology': 342,\n",
       " 'languange': 343,\n",
       " 'seamiest': 344,\n",
       " 'smilin': 345,\n",
       " 'allabsorbing': 346,\n",
       " 'tritone': 347,\n",
       " 'webern': 348,\n",
       " 'reintroductions': 349,\n",
       " 'occupationally': 350,\n",
       " 'psychothriller': 351,\n",
       " 'sampath': 352,\n",
       " 'bloodbrothers': 353,\n",
       " 'closest': 354,\n",
       " 'kefauver': 355,\n",
       " 'gainess': 356,\n",
       " 'presenting': 357,\n",
       " 'kerch': 358,\n",
       " 'ekgs': 359,\n",
       " 'triangle': 360,\n",
       " 'rachmaninov': 361,\n",
       " 'stanislavskys': 362,\n",
       " 'rosemans': 363,\n",
       " 'pseudopatriotic': 364,\n",
       " 'mcmanus': 365,\n",
       " 'specificities': 366,\n",
       " 'betimes': 367,\n",
       " 'relieved': 368,\n",
       " '1950s60': 369,\n",
       " 'servicebased': 370,\n",
       " 'genevas': 371,\n",
       " 'issure': 372,\n",
       " 'belligerant': 373,\n",
       " 'extirpated': 374,\n",
       " 'highlyentertaining': 375,\n",
       " 'sleptby': 376,\n",
       " 'branzillo': 377,\n",
       " 'peoplewatching': 378,\n",
       " 'forshadowed': 379,\n",
       " 'assualts': 380,\n",
       " 'jackhammers': 381,\n",
       " 'asas': 382,\n",
       " 'medicalindustrial': 383,\n",
       " 'hippa': 384,\n",
       " 'unfamilliar': 385,\n",
       " 'wheelman': 386,\n",
       " 'exgirl': 387,\n",
       " 'shakepeares': 388,\n",
       " 'neilan': 389,\n",
       " 'retyping': 390,\n",
       " 'buckhead': 391,\n",
       " 'alteredstate': 392,\n",
       " 'zardo': 393,\n",
       " 'differenct': 394,\n",
       " 'holberg': 395,\n",
       " 'deese': 396,\n",
       " 'unexplainably': 397,\n",
       " 'marketbuy': 398,\n",
       " 'vantreases': 399,\n",
       " 'blackingham': 400,\n",
       " 'derridean': 401,\n",
       " 'crumby': 402,\n",
       " 'nappis': 403,\n",
       " '11am': 404,\n",
       " 'bedfellow': 405,\n",
       " 'puryear': 406,\n",
       " 'salesy': 407,\n",
       " 'jadas': 408,\n",
       " 'msu': 409,\n",
       " 'abovereferenced': 410,\n",
       " 'weightbearing': 411,\n",
       " 'estrogenic': 412,\n",
       " 'contraints': 413,\n",
       " 'dilys': 414,\n",
       " 'nikkie': 415,\n",
       " 'proginoskes': 416,\n",
       " 'fingleton': 417,\n",
       " 'gingrich': 418,\n",
       " 'oiler': 419,\n",
       " 'whitson': 420,\n",
       " 'farradays': 421,\n",
       " 'reinemarie': 422,\n",
       " 'beauvior': 423,\n",
       " 'montana': 424,\n",
       " 'vasty': 425,\n",
       " 'capillary': 426,\n",
       " 'insync': 427,\n",
       " 'tembo': 428,\n",
       " 'sorens': 429,\n",
       " 'decrypting': 430,\n",
       " 'kistle': 431,\n",
       " 'astory': 432,\n",
       " 'vegis': 433,\n",
       " 'alpa': 434,\n",
       " 'chewier': 435,\n",
       " 'rathgeber': 436,\n",
       " 'lixia': 437,\n",
       " 'congee': 438,\n",
       " 'saginaw': 439,\n",
       " 'hayworths': 440,\n",
       " 'nutritionrelated': 441,\n",
       " 'mapleton': 442,\n",
       " 'lindelof': 443,\n",
       " 'trueness': 444,\n",
       " 'morton': 445,\n",
       " 'engram': 446,\n",
       " 'diiulio': 447,\n",
       " 'zoey': 448,\n",
       " 'liliuokalani': 449,\n",
       " 'blandford': 450,\n",
       " 'oathsworn': 451,\n",
       " 'p37': 452,\n",
       " 'storyfor': 453,\n",
       " 'boughtons': 454,\n",
       " 'gigatons': 455,\n",
       " 'brackston': 456,\n",
       " 'avowals': 457,\n",
       " 'kourtney': 458,\n",
       " 'helvenston': 459,\n",
       " 'bestperforming': 460,\n",
       " 'yalis': 461,\n",
       " 'mmp': 462,\n",
       " 'corvo': 463,\n",
       " 'manup': 464,\n",
       " 'dwi': 465,\n",
       " 'ultrarightwing': 466,\n",
       " 'ostlundh': 467,\n",
       " 'protagnist': 468,\n",
       " 'tenderizing': 469,\n",
       " 'muties': 470,\n",
       " 'habitations': 471,\n",
       " 'youknow': 472,\n",
       " 'ouellet': 473,\n",
       " 'sixhundredandfortythousand': 474,\n",
       " 'thengovernor': 475,\n",
       " 'waives': 476,\n",
       " 'bodenstein': 477,\n",
       " 'nele': 478,\n",
       " 'stefanies': 479,\n",
       " 'oppositesattract': 480,\n",
       " 'writerbut': 481,\n",
       " 'marinn': 482,\n",
       " 'kinbaku': 483,\n",
       " 'benot': 484,\n",
       " 'verylittle': 485,\n",
       " 'hsa': 486,\n",
       " 'herea': 487,\n",
       " 'moreabout': 488,\n",
       " 'maddisons': 489,\n",
       " 'payor': 490,\n",
       " 'borefest': 491,\n",
       " 'radha': 492,\n",
       " 'prosocialist': 493,\n",
       " 'siggy': 494,\n",
       " 'darkfriends': 495,\n",
       " 'vernor': 496,\n",
       " 'shayol': 497,\n",
       " 'insectile': 498,\n",
       " 'sharina': 499,\n",
       " 'barcas': 500,\n",
       " 'asmodean': 501,\n",
       " 'ashaman': 502,\n",
       " 'theprevious': 503,\n",
       " 'rhuidean': 504,\n",
       " 'slueths': 505,\n",
       " 'poetrythe': 506,\n",
       " 'delphiki': 507,\n",
       " 'sauscony': 508,\n",
       " 'everness': 509,\n",
       " 'atto': 510,\n",
       " 'adornettos': 511,\n",
       " 'lipsytes': 512,\n",
       " 'lethe': 513,\n",
       " 'lance': 514,\n",
       " 'etrade': 515,\n",
       " 'presales': 516,\n",
       " 'uggs': 517,\n",
       " 'otl': 518,\n",
       " 'hipped': 519,\n",
       " 'invalidism': 520,\n",
       " 'saxtons': 521,\n",
       " 'goodacre': 522,\n",
       " 'jkf': 523,\n",
       " 'undertreated': 524,\n",
       " 'paperbark': 525,\n",
       " 'tambora': 526,\n",
       " 'haudas': 527,\n",
       " 'joesbury': 528,\n",
       " 'octa': 529,\n",
       " 'mimick': 530,\n",
       " 'taming': 531,\n",
       " 'teensiest': 532,\n",
       " 'rachets': 533,\n",
       " 'stanfield': 534,\n",
       " 'taqiyya': 535,\n",
       " 'sharpssf': 536,\n",
       " 'bonansinga': 537,\n",
       " 'shanshan': 538,\n",
       " 'hydrostatic': 539,\n",
       " 'roti': 540,\n",
       " 'razorland': 541,\n",
       " 'surival': 542,\n",
       " 'goriness': 543,\n",
       " 'nightshifted': 544,\n",
       " 'backbut': 545,\n",
       " 'rosnays': 546,\n",
       " 'baumgartners': 547,\n",
       " 'smoosh': 548,\n",
       " 'blankfaced': 549,\n",
       " 'pyromaniacs': 550,\n",
       " 'bruhn': 551,\n",
       " 'unviable': 552,\n",
       " 'orrens': 553,\n",
       " 'speedups': 554,\n",
       " 'crackly': 555,\n",
       " 'fourleaf': 556,\n",
       " 'maclemoore': 557,\n",
       " 'reshuffle': 558,\n",
       " 'impulse': 559,\n",
       " 'freemarketeers': 560,\n",
       " 'pontelliers': 561,\n",
       " 'moreto': 562,\n",
       " 'noumena': 563,\n",
       " 'maneless': 564,\n",
       " 'maryjanice': 565,\n",
       " 'bratwurst': 566,\n",
       " 'camile': 567,\n",
       " 'marcheline': 568,\n",
       " 'geminis': 569,\n",
       " 'guatemalans': 570,\n",
       " 'percenters': 571,\n",
       " 'ruhollah': 572,\n",
       " 'f111': 573,\n",
       " 'ghwb': 574,\n",
       " 'meen': 575,\n",
       " 'nasally': 576,\n",
       " 'levene': 577,\n",
       " 'fini': 578,\n",
       " 'collen': 579,\n",
       " 'horrorstruck': 580,\n",
       " 'pooly': 581,\n",
       " 'throttles': 582,\n",
       " 'antarcticas': 583,\n",
       " 'jmt': 584,\n",
       " 'twothousandandforty-two': 585,\n",
       " 'flummery': 586,\n",
       " 'ohsomany': 587,\n",
       " 'amongs': 588,\n",
       " 'weasle': 589,\n",
       " 'hartnett': 590,\n",
       " 'nonviable': 591,\n",
       " 'carmella': 592,\n",
       " 'dysmorphic': 593,\n",
       " 'borroughs': 594,\n",
       " 'ultraorthodoxy': 595,\n",
       " 'hamrick': 596,\n",
       " 'chayil': 597,\n",
       " 'pinkus': 598,\n",
       " 'saught': 599,\n",
       " 'lcwr': 600,\n",
       " 'emmitsburg': 601,\n",
       " 'aliceinwonderland': 602,\n",
       " 'reallllly': 603,\n",
       " 'pinkwater': 604,\n",
       " 'conradian': 605,\n",
       " 'collections': 606,\n",
       " 'boop': 607,\n",
       " 'sugarville': 608,\n",
       " 'parfords': 609,\n",
       " 'blowjob': 610,\n",
       " 'brendan': 611,\n",
       " 'icequeen': 612,\n",
       " 'harlequinjunkie': 613,\n",
       " 'staying': 614,\n",
       " 'brazell': 615,\n",
       " 'romancy': 616,\n",
       " 'sitian': 617,\n",
       " 'equines': 618,\n",
       " 'severns': 619,\n",
       " 'iam': 620,\n",
       " 'strums': 621,\n",
       " 'higginss': 622,\n",
       " 'kresely': 623,\n",
       " 'elopements': 624,\n",
       " 'devlin': 625,\n",
       " 'pilby': 626,\n",
       " 'notsopleasant': 627,\n",
       " 'heartbeat': 628,\n",
       " 'colgan': 629,\n",
       " 'lonetree': 630,\n",
       " 'sherrifs': 631,\n",
       " 'ourselfs': 632,\n",
       " 'loisa': 633,\n",
       " 'nairna': 634,\n",
       " 'annjas': 635,\n",
       " 'cathartically': 636,\n",
       " 'weensy': 637,\n",
       " 'ladon': 638,\n",
       " 'conquor': 639,\n",
       " 'aerons': 640,\n",
       " 'rivenhall': 641,\n",
       " 'salford': 642,\n",
       " 'smythesewn': 643,\n",
       " 'addiciton': 644,\n",
       " 'unterwegers': 645,\n",
       " 'selfstimulation': 646,\n",
       " 'frazen': 647,\n",
       " 'storiesof': 648,\n",
       " 'sothat': 649,\n",
       " 'thassas': 650,\n",
       " 'telecommute': 651,\n",
       " 'fastaction': 652,\n",
       " 'inabilty': 653,\n",
       " 'misallocated': 654,\n",
       " 'joonas': 655,\n",
       " 'vascillates': 656,\n",
       " 'handstands': 657,\n",
       " 'ede': 658,\n",
       " 'pinol': 659,\n",
       " 'leake': 660,\n",
       " 'ameisen': 661,\n",
       " 'longchamp': 662,\n",
       " 'burned': 663,\n",
       " 'shelly': 664,\n",
       " 'acidophilus': 665,\n",
       " 'energ': 666,\n",
       " 'dietbook': 667,\n",
       " 'leones': 668,\n",
       " 'yvonne': 669,\n",
       " 'attendence': 670,\n",
       " 'tarpaper': 671,\n",
       " 'boyland': 672,\n",
       " 'telecasts': 673,\n",
       " 'caricaturelike': 674,\n",
       " 'surronding': 675,\n",
       " 'ironists': 676,\n",
       " 'semsandbergs': 677,\n",
       " 'inflammations': 678,\n",
       " 'palacios': 679,\n",
       " 'marcones': 680,\n",
       " 'westerneducated': 681,\n",
       " 'horserace': 682,\n",
       " 'brogues': 683,\n",
       " 'alainn': 684,\n",
       " 'merope': 685,\n",
       " 'leyers': 686,\n",
       " 'fazes': 687,\n",
       " 'witchiness': 688,\n",
       " 'supplicants': 689,\n",
       " 'plenimar': 690,\n",
       " 'klia': 691,\n",
       " 'ligation': 692,\n",
       " 'alljust': 693,\n",
       " 'jaime': 694,\n",
       " 'doovers': 695,\n",
       " 'sodomite': 696,\n",
       " 'scrappers': 697,\n",
       " 'tormod': 698,\n",
       " 'loveyou': 699,\n",
       " 'patrickgoudreau': 700,\n",
       " 'mayham': 701,\n",
       " 'hilzoy': 702,\n",
       " 'relized': 703,\n",
       " 'caspers': 704,\n",
       " 'hotel': 705,\n",
       " 'lallybroch': 706,\n",
       " 'ballons': 707,\n",
       " '5yr': 708,\n",
       " 'redicks': 709,\n",
       " 'chathrand': 710,\n",
       " 'naga': 711,\n",
       " 'pageturning': 712,\n",
       " 'dumba': 713,\n",
       " 'copyandpaste': 714,\n",
       " 'greedo': 715,\n",
       " 'waaaayyy': 716,\n",
       " 'foulplay': 717,\n",
       " '100calorie': 718,\n",
       " 'potentional': 719,\n",
       " 'stormys': 720,\n",
       " 'delarua': 721,\n",
       " 'darleigh': 722,\n",
       " 'propers': 723,\n",
       " 'vivaamarisata': 724,\n",
       " 'nametag': 725,\n",
       " 'joepa': 726,\n",
       " 'zzzzzzz': 727,\n",
       " 'mcquarrie': 728,\n",
       " 'rookie': 729,\n",
       " 'blackheath': 730,\n",
       " 'richerand': 731,\n",
       " 'sumness': 732,\n",
       " 'imbeds': 733,\n",
       " 'trustbuilding': 734,\n",
       " 'brno': 735,\n",
       " 'turrow': 736,\n",
       " 'nickies': 737,\n",
       " 'presentand': 738,\n",
       " 'mcinnis': 739,\n",
       " 'uppercuts': 740,\n",
       " 'jamb': 741,\n",
       " 'vegetarianfriendly': 742,\n",
       " 'likker': 743,\n",
       " 'slaws': 744,\n",
       " 'lanced': 745,\n",
       " 'renu': 746,\n",
       " 'strnad': 747,\n",
       " 'minutes': 748,\n",
       " 'minimumsecurity': 749,\n",
       " 'yarber': 750,\n",
       " 'nonathlete': 751,\n",
       " 'corban': 752,\n",
       " 'foodsand': 753,\n",
       " '8g': 754,\n",
       " 'manifesta': 755,\n",
       " 'renners': 756,\n",
       " 'mighthave': 757,\n",
       " 'bernadino': 758,\n",
       " 'singsing': 759,\n",
       " 'naohiro': 760,\n",
       " 'weaponization': 761,\n",
       " 'highlyrecommend': 762,\n",
       " 'viet': 763,\n",
       " 'periscos': 764,\n",
       " 'azeri': 765,\n",
       " 'fazioli': 766,\n",
       " 'quartier': 767,\n",
       " 'threatend': 768,\n",
       " 'texasstyle': 769,\n",
       " 'inaugurate': 770,\n",
       " 'tuesday': 771,\n",
       " 'drumont': 772,\n",
       " 'hayday': 773,\n",
       " 'heigh': 774,\n",
       " 'razorbacks': 775,\n",
       " 'yn': 776,\n",
       " 'conure': 777,\n",
       " 'starbursts': 778,\n",
       " 'mafiastyle': 779,\n",
       " 'blixs': 780,\n",
       " 'mouselike': 781,\n",
       " 'boxcutter': 782,\n",
       " 'skyisfalling': 783,\n",
       " 'sigmund': 784,\n",
       " 'osler': 785,\n",
       " 'alternadad': 786,\n",
       " 'icewater': 787,\n",
       " 'mcelvaines': 788,\n",
       " 'junkscience': 789,\n",
       " 'hjelms': 790,\n",
       " 'palawan': 791,\n",
       " 'hardright': 792,\n",
       " 'lowfunctioning': 793,\n",
       " 'ninetyminute': 794,\n",
       " 'nonnuclear': 795,\n",
       " 'algers': 796,\n",
       " 'camp': 797,\n",
       " 'lanya': 798,\n",
       " 'ramlogan': 799,\n",
       " 'cutglass': 800,\n",
       " 'mente': 801,\n",
       " 'gemini': 802,\n",
       " 'whish': 803,\n",
       " 'mainlines': 804,\n",
       " 'kingdon': 805,\n",
       " 'eurydike': 806,\n",
       " 'avante': 807,\n",
       " 'relato': 808,\n",
       " 'upone': 809,\n",
       " 'undertstand': 810,\n",
       " 'cyclotron': 811,\n",
       " 'martial': 812,\n",
       " 'polymorphous': 813,\n",
       " 'avedon': 814,\n",
       " 'regnault': 815,\n",
       " 'similarlooking': 816,\n",
       " 'castells': 817,\n",
       " 'markoe': 818,\n",
       " 'overgrazing': 819,\n",
       " 'baling': 820,\n",
       " 'dehli': 821,\n",
       " 'tramonto': 822,\n",
       " 'onmy': 823,\n",
       " 'journo': 824,\n",
       " 'baycol': 825,\n",
       " 'scandles': 826,\n",
       " 'rashad': 827,\n",
       " 'aladdins': 828,\n",
       " 'polonophobic': 829,\n",
       " 'godbold': 830,\n",
       " 'lousia': 831,\n",
       " 'life4': 832,\n",
       " 'maximising': 833,\n",
       " 'mgt': 834,\n",
       " 'lewys': 835,\n",
       " 'sinterklaas': 836,\n",
       " 'wellknit': 837,\n",
       " 'mothergoddess': 838,\n",
       " 'maggid': 839,\n",
       " 'nymphomaniacs': 840,\n",
       " 'agabus': 841,\n",
       " 'worthiest': 842,\n",
       " 'avsey': 843,\n",
       " 'karamazovs': 844,\n",
       " 'lipsmacking': 845,\n",
       " 'apas': 846,\n",
       " 'catsup': 847,\n",
       " 'recopies': 848,\n",
       " 'wineapples': 849,\n",
       " 'aylward': 850,\n",
       " 'jenkinson': 851,\n",
       " 'macmann': 852,\n",
       " 'specialforces': 853,\n",
       " 'planation': 854,\n",
       " 'critisizing': 855,\n",
       " 'nachtwey': 856,\n",
       " 'movabletype': 857,\n",
       " 'litres': 858,\n",
       " 'frisbees': 859,\n",
       " 'neoslavery': 860,\n",
       " 'amparo': 861,\n",
       " 'polically': 862,\n",
       " 'bombthrower': 863,\n",
       " 'multiuser': 864,\n",
       " 'margauxs': 865,\n",
       " 'usisraeli': 866,\n",
       " 'krasnansky': 867,\n",
       " 'joon': 868,\n",
       " 'glendinnings': 869,\n",
       " 'kansans': 870,\n",
       " 'heyman': 871,\n",
       " 'esztherhas': 872,\n",
       " 'toussaints': 873,\n",
       " 'jeanmichel': 874,\n",
       " 'spoonbread': 875,\n",
       " 'be2': 876,\n",
       " 'romancegenre': 877,\n",
       " 'tfotc': 878,\n",
       " 'internationalized': 879,\n",
       " 'bvm': 880,\n",
       " 'tocm': 881,\n",
       " 'zubin': 882,\n",
       " 'shadrack': 883,\n",
       " 'stromboli': 884,\n",
       " 'adb': 885,\n",
       " 'commodify': 886,\n",
       " 'landholding': 887,\n",
       " 'acts': 888,\n",
       " 'militiary': 889,\n",
       " 'themselfs': 890,\n",
       " 'enthusastic': 891,\n",
       " 'taite': 892,\n",
       " 'wil': 893,\n",
       " 'haygoods': 894,\n",
       " 'vakeel': 895,\n",
       " 'yezads': 896,\n",
       " 'orkneys': 897,\n",
       " 'depersonalizing': 898,\n",
       " 'anticonfederate': 899,\n",
       " 'warner': 900,\n",
       " 'phisical': 901,\n",
       " 'tightlycontrolled': 902,\n",
       " 'village': 903,\n",
       " 'guiltlessly': 904,\n",
       " 'nechayev': 905,\n",
       " 'moorhead': 906,\n",
       " 'eep': 907,\n",
       " 'greenlander': 908,\n",
       " 'inerva': 909,\n",
       " 'ghetel': 910,\n",
       " 'pertwee': 911,\n",
       " 'sigfrid': 912,\n",
       " 'pohls': 913,\n",
       " 'revelstone': 914,\n",
       " 'bloodguard': 915,\n",
       " 'casks': 916,\n",
       " 'chickenheads': 917,\n",
       " 'stillactive': 918,\n",
       " 'moodily': 919,\n",
       " 'reuel': 920,\n",
       " 'videssos': 921,\n",
       " 'discused': 922,\n",
       " 'tonios': 923,\n",
       " 'outofthebody': 924,\n",
       " 'englishisbn': 925,\n",
       " 'yuriy': 926,\n",
       " 'dokken': 927,\n",
       " 'druss': 928,\n",
       " 'lessas': 929,\n",
       " 'shanarra': 930,\n",
       " 'mankzin': 931,\n",
       " 'absurds': 932,\n",
       " 'eldar': 933,\n",
       " 'paddocks': 934,\n",
       " 'kinswoman': 935,\n",
       " 'doriath': 936,\n",
       " 'arwens': 937,\n",
       " 'mister': 938,\n",
       " 'astrogator': 939,\n",
       " 'infastructure': 940,\n",
       " 'robet': 941,\n",
       " 'ojo': 942,\n",
       " 'ringworld': 943,\n",
       " 'fueding': 944,\n",
       " 'emaleth': 945,\n",
       " 'mailer': 946,\n",
       " 'muchchanged': 947,\n",
       " 'iantine': 948,\n",
       " 'dragonet': 949,\n",
       " 'supermind': 950,\n",
       " 'shock': 951,\n",
       " 'elbryan': 952,\n",
       " 'dactyl': 953,\n",
       " 'ressurrection': 954,\n",
       " 'ifhe': 955,\n",
       " 'fundimental': 956,\n",
       " 'lifeextension': 957,\n",
       " 'devinci': 958,\n",
       " 'geas': 959,\n",
       " 'narcissm': 960,\n",
       " 'rivan': 961,\n",
       " 'selfauthentication': 962,\n",
       " 'pendulous': 963,\n",
       " 'unrepairable': 964,\n",
       " 'starkweather': 965,\n",
       " 'technosavvy': 966,\n",
       " 'knockknock': 967,\n",
       " 'lempriere': 968,\n",
       " 'fueds': 969,\n",
       " 'onethousandandfifty-eight': 970,\n",
       " 'crystalizes': 971,\n",
       " 'distressful': 972,\n",
       " 'donleavys': 973,\n",
       " 'starcave': 974,\n",
       " 'abase': 975,\n",
       " 'vestieri': 976,\n",
       " 'barfields': 977,\n",
       " 'tsarskoe': 978,\n",
       " 'asan': 979,\n",
       " 'conservations': 980,\n",
       " 'hells': 981,\n",
       " 'alethiometer': 982,\n",
       " 'miniencyclopedia': 983,\n",
       " 'shgall': 984,\n",
       " 'spoilersso': 985,\n",
       " 'raster': 986,\n",
       " '15years': 987,\n",
       " 'colorspace': 988,\n",
       " 'featurerich': 989,\n",
       " '170page': 990,\n",
       " 'windups': 991,\n",
       " 'openssh': 992,\n",
       " 'hijax': 993,\n",
       " 'newb': 994,\n",
       " 'toolusing': 995,\n",
       " 'lamothe': 996,\n",
       " 'aspx': 997,\n",
       " 'softboxes': 998,\n",
       " 'helloworld': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_dict = final_dic_df01['word'].to_dict()\n",
    "inv_filtered_dict = {v: k for k, v in filtered_dict.items()}\n",
    "inv_filtered_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_words(review):\n",
    "    new_review = []\n",
    "    for word in review:\n",
    "        word = word.strip()\n",
    "        if word in inv_filtered_dict:\n",
    "            new_review.append(word)\n",
    "    return new_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:: 100%|██████████| 582711/582711 [00:10<00:00, 54597.78it/s]\n"
     ]
    }
   ],
   "source": [
    "df_new_02 = df_new_01.assign(filteredText = df_new_01['reviewText'].progress_apply(lambda review:filter_words(review)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:: 100%|██████████| 582711/582711 [00:00<00:00, 954656.65it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>wordCountBefore</th>\n",
       "      <th>filteredText</th>\n",
       "      <th>wordCountAfter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>A2XQ5LZHTD4AFT</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>[timeless,  gibran,  backs,  content,  means, ...</td>\n",
       "      <td>49</td>\n",
       "      <td>[mouth, sail, messege]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>AF7CSSGV93RXN</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>[ prophet,  kahlil,  gibran,  thirty,  years, ...</td>\n",
       "      <td>19</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>A1NPNGWBVD9AK3</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>[ first,  books,  recall,  collection,  gibran...</td>\n",
       "      <td>74</td>\n",
       "      <td>[catechism, texts, siddhartha, contain, preach...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>A3IS4WGMFR4X65</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>[prophet,  kahlil,  work,  world,  million,  c...</td>\n",
       "      <td>142</td>\n",
       "      <td>[visions, emerson, critic, claude, intuition, ...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>AWLFVCT9128JV</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>[gibran,  khalil,  gibran,  born,  one thousan...</td>\n",
       "      <td>48</td>\n",
       "      <td>[strict, almustafa, manner]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>AFY0BT42DDYZV</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>[days,  gibrans,  gets,  literature,  yet,  bo...</td>\n",
       "      <td>177</td>\n",
       "      <td>[fame, twentysix, confidence, sane, drama, sag...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>A25P6DY6ARTCGZ</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>[book,  gibran,  took,  millions,  encapsulate...</td>\n",
       "      <td>29</td>\n",
       "      <td>[manner, existence, universal]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>A1SP45I55GQIIE</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>[ words,  kahlil,  gibran,  divine,  wisdom,  ...</td>\n",
       "      <td>35</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>A2E71VWXO59342</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>[prophet,  dispenses,  wisdom,  ones,  bids,  ...</td>\n",
       "      <td>30</td>\n",
       "      <td>[define, ability]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>A2OP1HD9RGX5OW</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>[book,  myth,  work,  beauty,  whose,  every, ...</td>\n",
       "      <td>42</td>\n",
       "      <td>[till, simplicity, gut, speaks]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>A2052JNVUPRTMT</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>[ gets,  bedrock,  man,  prophet,  anyone,  wo...</td>\n",
       "      <td>43</td>\n",
       "      <td>[signs]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>AGKPTMTR3UX1R</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>[kahlil,  eighteen million,  poet,  mystic,  n...</td>\n",
       "      <td>50</td>\n",
       "      <td>[finer, includes, spoke]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>A1HS49P9TZRGV9</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>[ book,  collection,  remember,  around,  twel...</td>\n",
       "      <td>68</td>\n",
       "      <td>[talks]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>A2ZZHMT58ZMVCZ</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>[prophet,  years,  ship,  back,  homeland,  kn...</td>\n",
       "      <td>111</td>\n",
       "      <td>[contain, pillars, exile]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>A3W43PSHRIG8KV</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>[ aware,  kahlil,  gibran,  read,  poem,  menu...</td>\n",
       "      <td>46</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>A1TR1LU2JSZLUL</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>[book,  gift,  journeyed,  overseas,  quest,  ...</td>\n",
       "      <td>39</td>\n",
       "      <td>[overseas, enlightenment, layers]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>ADIDQRLLR4KBQ</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>[atheist,  may,  seem,  strange,  people,  boo...</td>\n",
       "      <td>90</td>\n",
       "      <td>[holds, includes, states]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>A3AW2ZG0GP4SKN</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>[ book,  son,  stolen,  despair,  resonance,  ...</td>\n",
       "      <td>11</td>\n",
       "      <td>[souls]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>A2MMON52VMO7NT</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>[gibrans,  words,  lay,  bare,  simplicity,  t...</td>\n",
       "      <td>23</td>\n",
       "      <td>[simplicity]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>AR72Z89LACZ8Q</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>[ departure,  prophet,  people,  gather,  arou...</td>\n",
       "      <td>26</td>\n",
       "      <td>[gather]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            userId        asin  \\\n",
       "0   A2XQ5LZHTD4AFT  000100039X   \n",
       "1    AF7CSSGV93RXN  000100039X   \n",
       "2   A1NPNGWBVD9AK3  000100039X   \n",
       "3   A3IS4WGMFR4X65  000100039X   \n",
       "4    AWLFVCT9128JV  000100039X   \n",
       "5    AFY0BT42DDYZV  000100039X   \n",
       "6   A25P6DY6ARTCGZ  000100039X   \n",
       "7   A1SP45I55GQIIE  000100039X   \n",
       "8   A2E71VWXO59342  000100039X   \n",
       "9   A2OP1HD9RGX5OW  000100039X   \n",
       "10  A2052JNVUPRTMT  000100039X   \n",
       "11   AGKPTMTR3UX1R  000100039X   \n",
       "12  A1HS49P9TZRGV9  000100039X   \n",
       "13  A2ZZHMT58ZMVCZ  000100039X   \n",
       "14  A3W43PSHRIG8KV  000100039X   \n",
       "15  A1TR1LU2JSZLUL  000100039X   \n",
       "16   ADIDQRLLR4KBQ  000100039X   \n",
       "17  A3AW2ZG0GP4SKN  000100039X   \n",
       "18  A2MMON52VMO7NT  000100039X   \n",
       "19   AR72Z89LACZ8Q  000100039X   \n",
       "\n",
       "                                           reviewText  wordCountBefore  \\\n",
       "0   [timeless,  gibran,  backs,  content,  means, ...               49   \n",
       "1   [ prophet,  kahlil,  gibran,  thirty,  years, ...               19   \n",
       "2   [ first,  books,  recall,  collection,  gibran...               74   \n",
       "3   [prophet,  kahlil,  work,  world,  million,  c...              142   \n",
       "4   [gibran,  khalil,  gibran,  born,  one thousan...               48   \n",
       "5   [days,  gibrans,  gets,  literature,  yet,  bo...              177   \n",
       "6   [book,  gibran,  took,  millions,  encapsulate...               29   \n",
       "7   [ words,  kahlil,  gibran,  divine,  wisdom,  ...               35   \n",
       "8   [prophet,  dispenses,  wisdom,  ones,  bids,  ...               30   \n",
       "9   [book,  myth,  work,  beauty,  whose,  every, ...               42   \n",
       "10  [ gets,  bedrock,  man,  prophet,  anyone,  wo...               43   \n",
       "11  [kahlil,  eighteen million,  poet,  mystic,  n...               50   \n",
       "12  [ book,  collection,  remember,  around,  twel...               68   \n",
       "13  [prophet,  years,  ship,  back,  homeland,  kn...              111   \n",
       "14  [ aware,  kahlil,  gibran,  read,  poem,  menu...               46   \n",
       "15  [book,  gift,  journeyed,  overseas,  quest,  ...               39   \n",
       "16  [atheist,  may,  seem,  strange,  people,  boo...               90   \n",
       "17  [ book,  son,  stolen,  despair,  resonance,  ...               11   \n",
       "18  [gibrans,  words,  lay,  bare,  simplicity,  t...               23   \n",
       "19  [ departure,  prophet,  people,  gather,  arou...               26   \n",
       "\n",
       "                                         filteredText  wordCountAfter  \n",
       "0                              [mouth, sail, messege]               3  \n",
       "1                                                  []               0  \n",
       "2   [catechism, texts, siddhartha, contain, preach...               8  \n",
       "3   [visions, emerson, critic, claude, intuition, ...               7  \n",
       "4                         [strict, almustafa, manner]               3  \n",
       "5   [fame, twentysix, confidence, sane, drama, sag...               9  \n",
       "6                      [manner, existence, universal]               3  \n",
       "7                                                  []               0  \n",
       "8                                   [define, ability]               2  \n",
       "9                     [till, simplicity, gut, speaks]               4  \n",
       "10                                            [signs]               1  \n",
       "11                           [finer, includes, spoke]               3  \n",
       "12                                            [talks]               1  \n",
       "13                          [contain, pillars, exile]               3  \n",
       "14                                                 []               0  \n",
       "15                  [overseas, enlightenment, layers]               3  \n",
       "16                          [holds, includes, states]               3  \n",
       "17                                            [souls]               1  \n",
       "18                                       [simplicity]               1  \n",
       "19                                           [gather]               1  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new_03 = df_new_02.assign(wordCountAfter = df_new_02['filteredText'].progress_apply(lambda review:len(review)))\n",
    "df_new_03[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining = 1 - df_new_03['wordCountAfter'].sum() / df_new_03['wordCountBefore'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average noun reduction achieved:94.83271612887818%\n"
     ]
    }
   ],
   "source": [
    "print(\"Average noun reduction achieved:\" + str(remaining*100) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Association Rules Mining Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:: 100%|██████████| 59324/59324 [00:04<00:00, 14441.45it/s]\n",
      "Progress:: 100%|██████████| 59324/59324 [00:00<00:00, 1036502.92it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>filteredText</th>\n",
       "      <th>transactions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>[[mouth, sail, messege], [], [catechism, texts...</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0002051850</td>\n",
       "      <td>[[montana, root, thee, thou, cause, cause], [h...</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0002113570</td>\n",
       "      <td>[[], [continues, usfor, continues], [observati...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0002117088</td>\n",
       "      <td>[[goodnight, therapy, claude, sunny, claude, s...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>000215725X</td>\n",
       "      <td>[[], [experts], [authority, perpetual, intervi...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin                                       filteredText  transactions\n",
       "0  000100039X  [[mouth, sail, messege], [], [catechism, texts...            30\n",
       "1  0002051850  [[montana, root, thee, thou, cause, cause], [h...            31\n",
       "2  0002113570  [[], [continues, usfor, continues], [observati...             7\n",
       "3  0002117088  [[goodnight, therapy, claude, sunny, claude, s...             5\n",
       "4  000215725X  [[], [experts], [authority, perpetual, intervi...            11"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_books_bigReviews = pd.DataFrame(df_new_03[['asin','filteredText']].groupby(['asin'])['filteredText'].progress_apply(list))\n",
    "df_books_bigReviews = df_books_bigReviews.reset_index()\n",
    "df_books_bigReviews = df_books_bigReviews.assign(transactions = df_books_bigReviews['filteredText'].progress_apply(lambda reviews_lis:len(reviews_lis)))\n",
    "df_books_bigReviews.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from apyori import apriori\n",
    "\n",
    "# Support\n",
    "# Support is an indication of how frequently the itemset appears in the dataset.\n",
    "# Confidence\n",
    "# Confidence is an indication of how often the rule has been found to be true.\n",
    "# Lift\n",
    "# The ratio of the observed support to that expected if X and Y were independent.\n",
    "def apply_arm(transactions):\n",
    "    return list(apriori(transactions, min_support = 1/len(transactions), min_confidence = 1, min_lift = len(transactions), max_length = 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress::   9%|▉         | 5403/59324 [4:23:32<32:38:14,  2.18s/it]    "
     ]
    }
   ],
   "source": [
    "books_with_arm = df_books_bigReviews.assign(arm = df_books_bigReviews['filteredText'].progress_apply(lambda list_of_reviews:apply_arm(list_of_reviews)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_with_arm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_important_nouns(arms):\n",
    "    imp_nns = []\n",
    "    if \"items\" in pd.DataFrame(arms).keys():\n",
    "        results = list(pd.DataFrame(arms)['items'])\n",
    "        for result in results:\n",
    "            if len(list(result)) > 4:\n",
    "                imp_nns = imp_nns + list(list(result))\n",
    "        if(len(imp_nns)==0):\n",
    "            for result in results:\n",
    "                if len(list(result)) > 3:\n",
    "                    imp_nns = imp_nns + list(list(result))            \n",
    "        return list(set(imp_nns))\n",
    "    return list(set(imp_nns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'books_with_arm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-30-b51a9e51ea9c>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m imp_nns_df = books_with_arm.assign(imp_nns = books_with_arm['arm']\n\u001B[0m\u001B[1;32m      2\u001B[0m                                    .progress_apply(lambda arms:get_important_nouns(arms)))\n\u001B[1;32m      3\u001B[0m \u001B[0mimp_nns_df\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhead\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'books_with_arm' is not defined"
     ]
    }
   ],
   "source": [
    "imp_nns_df = books_with_arm.assign(imp_nns = books_with_arm['arm']\n",
    "                                   .progress_apply(lambda arms:get_important_nouns(arms)))\n",
    "imp_nns_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'imp_nns_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-31-2f3b7b986817>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mimp_nns_df\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mimp_nns_df\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'asin'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m'imp_nns'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mimp_nns_df\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhead\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'imp_nns_df' is not defined"
     ]
    }
   ],
   "source": [
    "imp_nns_df = imp_nns_df[['asin','imp_nns']]\n",
    "imp_nns_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'imp_nns_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-32-5b3130299233>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mimp_nns_df\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto_pickle\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"../data/interim/005_important_nouns.p\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m: name 'imp_nns_df' is not defined"
     ]
    }
   ],
   "source": [
    "imp_nns_df.to_pickle(\"../data/interim/005_important_nouns.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "imp_nns_df = imp_nns_df.assign(num_of_imp_nouns = imp_nns_df['imp_nns'].progress_apply(lambda imp_nouns:len(imp_nouns)))\n",
    "imp_nns_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some more stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "For visuals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#%pip install python-decouple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from decouple import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "API_USERNAME = config('USER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "API_KEY = config('PLOTLY_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import chart_studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "chart_studio.tools.set_credentials_file(username=API_USERNAME, api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import chart_studio.plotly as py\n",
    "import plotly.offline\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import cufflinks as cf\n",
    "cf.go_offline()\n",
    "# Configure cufflings \n",
    "cf.set_config_file(offline=False, world_readable=True, theme='pearl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Filter out synonyms again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "booksWithNoImportantNouns = imp_nns_df.loc[imp_nns_df['num_of_imp_nouns'] == 0]\n",
    "len(booksWithNoImportantNouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "booksWithNoImportantNouns = imp_nns_df.loc[imp_nns_df['num_of_imp_nouns'] != 0]\n",
    "len(booksWithNoImportantNouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "booksWithNoImportantNouns[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "booksWithNoImportantNouns['num_of_imp_nouns'].iplot(kind='histogram', bins=100, xTitle='Number of Important Nouns', yTitle='Number of Books')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "booksWithNoImportantNouns.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# END OF FILE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}